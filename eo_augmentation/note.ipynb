{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from transformers import BertTokenizer\n",
    "from collections import namedtuple\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import math\n",
    "from neural_jacana.model import *\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_neural_jacana import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_texts(texts):\n",
    "    tokenized_texts = []\n",
    "    for text in texts:\n",
    "        tokenized_texts.append(nltk.word_tokenize(re.sub(r'[\\(\\)\\`\\'\\\"]', '', text)))\n",
    "    return tokenized_texts\n",
    "\n",
    "def get_unique_list(seq):\n",
    "    seen = []\n",
    "    return [x for x in seq if x not in seen and not seen.append(x)]\n",
    "\n",
    "def check_inclusion(list1, list2):\n",
    "    flag = False\n",
    "    for i in list1:\n",
    "        if i in list2:\n",
    "            flag = True\n",
    "    for i in list2:\n",
    "        if i in list1:\n",
    "            flag = True\n",
    "    return flag\n",
    "\n",
    "def merge_sent2_ids(align_id_pairs):\n",
    "    '''\n",
    "    merge sent2 ids aligned to sent1 ids.\n",
    "    '''\n",
    "    merged_sent2_align_id_pairs = []\n",
    "    sorted_align_id_pairs = sorted(align_id_pairs, key=lambda x:(int(re.findall(r\"\\d+\", x)[0]), int(re.findall(r\"\\d+\", x)[1])))\n",
    "    tuple_pairs = [(int(re.findall(r\"\\d+\", i)[0]), int(re.findall(r\"\\d+\", i)[1])) for i in sorted_align_id_pairs]\n",
    "    for pair in tuple_pairs:\n",
    "        keys = [i[0] for i in merged_sent2_align_id_pairs]\n",
    "        keys_flatten = [x for row in keys for x in row]\n",
    "        if pair[0] not in keys_flatten:\n",
    "            merged_sent2_align_id_pairs.append(([pair[0]], [pair[1]]))\n",
    "        else:\n",
    "            ind_addval = [i for i in range(len(keys)) if pair[0] in keys[i]][0]\n",
    "            merged_sent2_align_id_pairs[ind_addval][1].append(pair[1])\n",
    "    return merged_sent2_align_id_pairs\n",
    "\n",
    "def merge_sent1_ids(merged_sent2_align_id_pairs):\n",
    "    '''\n",
    "    merge sent1 ids having the same sent2 ids.\n",
    "    '''\n",
    "    merged_sent1_align_id_pairs = []\n",
    "    dup_inds = []\n",
    "    vals = [pair[1] for pair in merged_sent2_align_id_pairs]\n",
    "    for pair in merged_sent2_align_id_pairs:\n",
    "        dup_ind = [i for i, x in enumerate(vals) if x == pair[1]]\n",
    "        if len(dup_ind) > 1:\n",
    "            dup_inds.append(dup_ind)\n",
    "    dup_inds = get_unique_list(dup_inds)\n",
    "\n",
    "    if len(dup_inds) != 0: #if there are duplicate values in merged_sent2_align_id_pairs, they should be merged.\n",
    "        keys_to_add = []\n",
    "        for i in range(len(dup_inds)):\n",
    "            key_to_add = []\n",
    "            for j in range(len(merged_sent2_align_id_pairs)):\n",
    "                if j in dup_inds[i]:\n",
    "                    key_to_add.append(merged_sent2_align_id_pairs[j][0][0])\n",
    "            if len(key_to_add) != 0:\n",
    "                keys_to_add.append(key_to_add)\n",
    "        \n",
    "        pairs_to_add = []\n",
    "        for i in range(len(dup_inds)):\n",
    "            pairs_to_add.append((keys_to_add[i], merged_sent2_align_id_pairs[dup_inds[i][0]][1]))\n",
    "\n",
    "        dup_inds_flatten = [x for row in dup_inds for x in row]\n",
    "        for i in range(len(merged_sent2_align_id_pairs)):\n",
    "            if i not in dup_inds_flatten:\n",
    "                merged_sent1_align_id_pairs.append(merged_sent2_align_id_pairs[i])\n",
    "        merged_sent1_align_id_pairs.extend(pairs_to_add)\n",
    "        return merged_sent1_align_id_pairs\n",
    "    \n",
    "    else:\n",
    "        return merged_sent2_align_id_pairs\n",
    "\n",
    "def merge_align_ids_crossing(merged_ids):\n",
    "    sent1_ids = [pair[0] for pair in merged_ids]\n",
    "    sent2_ids = [pair[1] for pair in merged_ids]\n",
    "    res = []\n",
    "    added_sent1 = [0 for i in range(len(sent1_ids))]\n",
    "    for i in range(len(sent1_ids)):\n",
    "        sent2_ids_to_add = sent2_ids[i]\n",
    "        sent1_correspond = sent1_ids[i]\n",
    "        for j in range(i, len(sent1_ids)):\n",
    "            if check_inclusion(sent1_ids[i], sent1_ids[j]) == True:\n",
    "                if added_sent1[j] == 0 and i != j:\n",
    "                    added_sent1[j] = 1\n",
    "                    sent2_ids_to_add.extend(sent2_ids[j])\n",
    "                    sent1_correspond.extend(sent1_ids[j])\n",
    "                    sent1_correspond = get_unique_list(sent1_correspond)\n",
    "        if len(sent2_ids_to_add) > 1 and added_sent1 == 0:\n",
    "            res.append((sent1_correspond, sent2_ids_to_add))\n",
    "            added_sent1[i] = 1\n",
    "            #print(0)\n",
    "\n",
    "    added_sent2 = [0 for i in range(len(sent2_ids))]\n",
    "    for i in range(len(sent2_ids)):\n",
    "        sent1_ids_to_add = sent1_ids[i]\n",
    "        sent2_correspond = sent2_ids[i]\n",
    "        for j in range(i, len(sent2_ids)):\n",
    "            if check_inclusion(sent2_ids[i], sent2_ids[j]) == True:\n",
    "                if added_sent2[j] == 0 and i != j:\n",
    "                    added_sent2[j] = 1\n",
    "                    sent1_ids_to_add.extend(sent1_ids[j])\n",
    "                    sent2_correspond.extend(sent2_ids[j])\n",
    "                    sent2_correspond = get_unique_list(sent2_correspond)\n",
    "        if len(sent1_ids_to_add) > 1 and added_sent2[i] == 0:\n",
    "            res.append((sent1_ids_to_add, sent2_correspond))\n",
    "            added_sent2[i] = 1\n",
    "            #print(1)\n",
    "\n",
    "    for i in range(len(merged_ids)):\n",
    "        if added_sent1[i] == 0 and added_sent2[i] == 0:\n",
    "            res.append((sent1_ids[i], sent2_ids[i]))\n",
    "            #print(2)\n",
    "    #print(added_sent1, added_sent2)\n",
    "    return res\n",
    "\n",
    "def ids_to_words(merged_id_pairs, tokenized_sent1, tokenized_sent2):\n",
    "    align_word_pairs = []\n",
    "    for pair in merged_id_pairs:\n",
    "        sent1_words = [tokenized_sent1[i] for i in pair[0]]\n",
    "        sent2_words = [tokenized_sent2[i] for i in pair[1]]\n",
    "        align_word_pairs.append((sent1_words, sent2_words))\n",
    "    return align_word_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ['0-0', '1-0', '2-0', '1-1', '2-2', '3-3']\n",
    "a = ['0-0', '1-0', '1-1', '2-0', '2-2', '3-3', '3-4', '3-5', '4-4', '5-5', '6-6', '7-7']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([0], [0]),\n",
       " ([1], [0, 1]),\n",
       " ([2], [0, 2]),\n",
       " ([3], [3, 4, 5]),\n",
       " ([4], [4]),\n",
       " ([5], [5]),\n",
       " ([6], [6]),\n",
       " ([7], [7])]"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_sent2_ids(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([0, 1, 2], [0, 1, 2]), ([3, 4, 5], [3, 4, 5]), ([6], [6]), ([7], [7])]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_align_ids_crossing(merge_sent1_ids(merge_sent2_ids(a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yamanaka.h.ac/.pyenv/versions/3.8.5/lib/python3.8/site-packages/pandas/compat/__init__.py:97: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "system_rating = pd.read_csv('./src/simplicity_DA.csv')\n",
    "sources = []\n",
    "targets = []\n",
    "simp_zscores = []\n",
    "for sid in range(1, 360):\n",
    "    now = system_rating[system_rating['sent_id'] == sid]\n",
    "    if len(now) != 0:\n",
    "        sources.append(now['orig_sent'].iloc[0])\n",
    "        targets.append(now['simp_sent'].iloc[0])\n",
    "        simp_zscores.append(now['simplicity_zscore'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at neural_jacana/spanbert_hf_base and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/yamanaka.h.ac/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed test examples 0/1\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "6",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-bc73bcc1b244>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0minput_ids_a_and_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids_b_and_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_ids_a_and_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_ids_b_and_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent1_valid_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent2_valid_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent1_wordpiece_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent2_wordpiece_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         decoded_results = model(input_ids_a_and_b=input_ids_a_and_b, input_ids_b_and_a=input_ids_b_and_a,\n\u001b[0m\u001b[1;32m     69\u001b[0m                                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids_a_and_b\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msegment_ids_a_and_b\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                                     \u001b[0mtoken_type_ids_b_and_a\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msegment_ids_b_and_a\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.5/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/github_repos/simpeval/brute_force_edit_operation/neural_jacana/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids_a_and_b, input_ids_b_and_a, attention_mask, token_type_ids_a_and_b, token_type_ids_b_and_a, sent1_valid_ids, sent2_valid_ids, sent1_wordpiece_length, sent2_wordpiece_length, labels_s2t, labels_t2s)\u001b[0m\n\u001b[1;32m    491\u001b[0m                 \u001b[0mtag_masks_sentA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_tags_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_tags_A\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent2_lengths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 6"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--batchsize\", default=1, type=int)\n",
    "parser.add_argument(\"--learning_rate\", default=1e-5, type=float)\n",
    "parser.add_argument(\"--max_epoch\", default=6, type=int)\n",
    "parser.add_argument(\"--max_span_size\", default=4, type=int)\n",
    "parser.add_argument(\"--max_seq_length\", default=128, type=int)\n",
    "parser.add_argument(\"--max_sent_length\", default=70, type=int)\n",
    "parser.add_argument(\"--seed\", default=1234, type=int)\n",
    "parser.add_argument(\"--dataset\", default='mtref', type=str)\n",
    "parser.add_argument(\"--sure_and_possible\", default='True', type=str)\n",
    "parser.add_argument(\"--distance_embedding_size\", default=128, type=int)\n",
    "parser.add_argument(\"--use_transition_layer\", default='False', type=str, help='if False, will set transition score to 0.')\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = NeuralWordAligner(args)\n",
    "my_device = torch.device('cpu')\n",
    "model = model.to(my_device)\n",
    "\n",
    "checkpoint = torch.load('./neural_jacana/Checkpoint_sure_and_possible_True_dataset_mtref_batchsize_1_max_span_size_4_use_transition_layer_False_epoch_2_0.9150.pt', map_location=my_device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "#sources = [\"Military experts say the line between combat is getting blurry.\", \"Their eyes are quite small, and their visual acuity is poor.\", \n",
    "#            \"According to Ledford, Northrop executives said they would build substantial parts of the bomber in Palmdale, creating about 1,500 jobs.\",\n",
    "#            \"In return, Rollo swore fealty to Charles, converted to Christianity, and undertook to defend the northern region of France against the Incursions of other Viking groups.\",\n",
    "#            \"A fee is the price one pays as remuneration for services, especially the Honorarium paid to a doctor, lawyer, consultant, or other member of a learned profession.\",\n",
    "#            \"Thereafter the county's administration was conducted at Duns or Lauder until Greenlaw became the county town in 1596.\",\n",
    "#            \"MacGruber starts asking for simple objects to make something to defuse the bomb, but he is later distracted by something (usually involving his personal life) that makes him run out of time.\",\n",
    "#            \"As the largest sub-region in Mesoamerica, it encompassed a vast and varied landscape, from the mountainous regions of the Sierra Madre to the semi-arid plains of northern Yucatán.\",\n",
    "#            \"Together they formed New Music Manchester, a group committed to contemporary music.\"]\n",
    "#targets = [\"Military experts say war is changing.\", \"Their eyes are very small, and they do not see well.\",\n",
    "#            \"According to Ledford, Northrop said they would build most of the bomber parts in Palmdale. It would create 1,500 jobs.\",\n",
    "#            \"Rollo swore to be loyal to Charles, then he changed his religion to Christianity. Rollo protected northern France by fighting Viking invaders.\",\n",
    "#            \"A price one might pay for services is a called a fee.\",\n",
    "#            \"After that, the county offices were at Duns or Lauder. In 1596 they moved to Greenlaw.\",\n",
    "#            \"Macgruber starts by asking for simple objects to stop the bomb from working. later he is distracted by an event from his personal life. as a result, he runs out of time to stop the bomb.\",\n",
    "#            \"As the largest sub-region in Mesoamerica, it was a vast and varied landscape.\",\n",
    "#            \"Both of the formed a group committed to contemporary music called new music Manchester.\"]\n",
    "#sources = [\"As the largest sub-region in Mesoamerica, it encompassed a vast and varied landscape, from the mountainous regions of the Sierra Madre to the semi-arid plains of northern Yucatán.\"]\n",
    "#targets = [\"As the largest sub-region in Mesoamerica, it was a vast and varied landscape.\"]\n",
    "#sources = ['Together they formed New Music Manchester, a group committed to contemporary music.']\n",
    "#targets = ['Both of the formed a group committed to contemporary music called new music Manchester.']\n",
    "#sources = ['Characteristics Radar observations indicate a fairly pure iron-nickel composition.']\n",
    "#targets = ['A mainly pure Iron-Nickel composition was observed by radar.']\n",
    "#sources = [\"MacGruber starts asking for simple objects to make something to defuse the bomb, but he is later distracted by something (usually involving his personal life) that makes him run out of time.\"]\n",
    "#targets = [\"Macgruber starts by asking for simple objects to stop the bomb from working. later he is distracted by an event from his personal life. as a result, he runs out of time to stop the bomb.\"]\n",
    "sources = ['It continues as the Bohemian Switzerland in the Czech Republic.']\n",
    "targets = ['It continues.']\n",
    "nltk.download('punkt')\n",
    "tokenized_sources = preprocess_texts(sources)\n",
    "tokenized_targets = preprocess_texts(targets)\n",
    "\n",
    "data = []\n",
    "example = namedtuple('example', 'ID, text_a, text_b, label')\n",
    "for i, (tokenized_source, tokenized_target) in enumerate(zip(tokenized_sources, tokenized_targets)):\n",
    "    data.append(example(i, ' '.join(tokenized_source), ' '.join(tokenized_target), '0-0'))\n",
    "test_dataloader = create_Data_Loader(data_examples=data, args=args, set_type='test', batchsize=1, max_seq_length=128, tokenizer=tokenizer)\n",
    "\n",
    "for step, batch in enumerate(test_dataloader):\n",
    "    batch = tuple(t.to(my_device) for t in batch)\n",
    "    input_ids_a_and_b, input_ids_b_and_a, input_mask, segment_ids_a_and_b, segment_ids_b_and_a, sent1_valid_ids, sent2_valid_ids, sent1_wordpiece_length, sent2_wordpiece_length = batch\n",
    "    with torch.no_grad():\n",
    "        decoded_results = model(input_ids_a_and_b=input_ids_a_and_b, input_ids_b_and_a=input_ids_b_and_a,\n",
    "                                    attention_mask=input_mask, token_type_ids_a_and_b=segment_ids_a_and_b,\n",
    "                                    token_type_ids_b_and_a=segment_ids_b_and_a,\n",
    "                                    sent1_valid_ids=sent1_valid_ids, sent2_valid_ids=sent2_valid_ids,\n",
    "                                    sent1_wordpiece_length=sent1_wordpiece_length,\n",
    "                                    sent2_wordpiece_length=sent2_wordpiece_length)\n",
    "    align_id_pairs = list(decoded_results[0])\n",
    "    #print(align_id_pairs)\n",
    "    merged_sent2_align_id_pairs = merge_sent2_ids(align_id_pairs)\n",
    "    merged_sent1_align_id_pairs = merge_sent1_ids(merged_sent2_align_id_pairs)\n",
    "    merged_id_pairs = merge_align_ids_crossing(merged_sent1_align_id_pairs)\n",
    "    align_word_pairs = ids_to_words(merged_id_pairs, tokenized_sources[step], tokenized_targets[step])\n",
    "    #print(merged_id_pairs, align_word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KeyError                                  Traceback (most recent call last)\n",
    "<ipython-input-8-2b41582f4fa5> in <module>\n",
    "     64     input_ids_a_and_b, input_ids_b_and_a, input_mask, segment_ids_a_and_b, segment_ids_b_and_a, sent1_valid_ids, sent2_valid_ids, sent1_wordpiece_length, sent2_wordpiece_length = batch\n",
    "     65     with torch.no_grad():\n",
    "---> 66         decoded_results = model(input_ids_a_and_b=input_ids_a_and_b, input_ids_b_and_a=input_ids_b_and_a,\n",
    "     67                                     attention_mask=input_mask, token_type_ids_a_and_b=segment_ids_a_and_b,\n",
    "     68                                     token_type_ids_b_and_a=segment_ids_b_and_a,\n",
    "\n",
    "~/.pyenv/versions/3.8.5/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)\n",
    "    887             result = self._slow_forward(*input, **kwargs)\n",
    "    888         else:\n",
    "--> 889             result = self.forward(*input, **kwargs)\n",
    "    890         for hook in itertools.chain(\n",
    "    891                 _global_forward_hooks.values(),\n",
    "\n",
    "~/Desktop/github_repos/simpeval/brute_force_edit_operation/neural_jacana/model.py in forward(self, input_ids_a_and_b, input_ids_b_and_a, attention_mask, token_type_ids_a_and_b, token_type_ids_b_and_a, sent1_valid_ids, sent2_valid_ids, sent1_wordpiece_length, sent2_wordpiece_length, labels_s2t, labels_t2s)\n",
    "    491                 for d in range(batch_size):\n",
    "    492                         extended_length_B = self.args.max_span_size*sent2_lengths[d]-int(self.args.max_span_size*(self.args.max_span_size-1)/2)\n",
    "--> 493                         batch_dist_idx_list_B[d,:(extended_length_B+1),:(extended_length_B+1)] = self.transition_matrix_dict[extended_length_B]\n",
    "    494                         tag_masks_sentB[d,:(extended_length_B+1),:(extended_length_B+1)]=np.ones((extended_length_B+1, extended_length_B+1))\n",
    "    495 \n",
    "\n",
    "KeyError: 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n",
      "It continues as the Bohemian Switzerland in the Czech Republic.\n",
      "It continues.\n"
     ]
    }
   ],
   "source": [
    "print(step)\n",
    "print(sources[step])\n",
    "print(targets[step])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_distance(sent1, sent2, max_id=4999):\n",
    "    m = len(sent1)\n",
    "    n = len(sent2)\n",
    "    dp = [[0 for x in range(n+1)] for x in range(m+1)]\n",
    "    for i in range(m+1):\n",
    "        for j in range(n+1):\n",
    "            if i == 0:\n",
    "                dp[i][j] = j    # Min. operations = j\n",
    "            elif j == 0:\n",
    "                dp[i][j] = i    # Min. operations = i\n",
    "            elif sent1[i-1].lower() == sent2[j-1].lower():\n",
    "                dp[i][j] = dp[i-1][j-1]\n",
    "            else:\n",
    "                edit_candidates = np.array([\n",
    "                    dp[i][j-1], # Insert\n",
    "                    dp[i-1][j] # Remove\n",
    "                    ])\n",
    "                dp[i][j] = 1 + min(edit_candidates)\n",
    "    return dp\n",
    "\n",
    "def sent2edit(sent1, sent2):\n",
    "    dp = edit_distance(sent1, sent2)\n",
    "    edits = []\n",
    "    pos = []\n",
    "    m, n = len(sent1), len(sent2)\n",
    "    while m != 0 or n != 0:\n",
    "        curr = dp[m][n]\n",
    "        if m==0: #have to insert all here\n",
    "            while n>0:\n",
    "                left = dp[1][n-1]\n",
    "                edits.append(sent2[n-1])\n",
    "                pos.append(left)\n",
    "                n-=1\n",
    "        elif n==0:\n",
    "            while m>0:\n",
    "                top = dp[m-1][n]\n",
    "                edits.append('DEL')\n",
    "                pos.append(top)\n",
    "                m -=1\n",
    "        else: # we didn't reach any special cases yet\n",
    "            diag = dp[m-1][n-1]\n",
    "            left = dp[m][n-1]\n",
    "            top = dp[m-1][n]\n",
    "            if sent2[n-1].lower() == sent1[m-1].lower(): # keep\n",
    "                edits.append('KEEP')\n",
    "                pos.append(diag)\n",
    "                m -= 1\n",
    "                n -= 1\n",
    "            elif curr == top+1: # INSERT preferred before DEL\n",
    "                edits.append('DEL')\n",
    "                pos.append(top)  # (sent2[n-1])\n",
    "                m -= 1\n",
    "            else: #insert\n",
    "                edits.append(sent2[n - 1])\n",
    "                pos.append(left)  # (sent2[n-1])\n",
    "                n -= 1\n",
    "    edits = edits[::-1]\n",
    "    return edits\n",
    "\n",
    "\n",
    "def edit2sent(sent, edits, last=False):\n",
    "    new_sent = []\n",
    "    sent_pointer = 0 #counter the total of KEEP and DEL, then align with original sentence\n",
    "    if len(edits) == 0 or len(sent) ==0: # edit_list empty, return original sent\n",
    "        return sent\n",
    "    for i, edit in enumerate(edits):\n",
    "        if len(sent) > sent_pointer: #there are tokens left for editing\n",
    "            if edit ==\"KEEP\":\n",
    "                new_sent.append(sent[sent_pointer])\n",
    "                sent_pointer += 1\n",
    "            elif edit ==\"DEL\":\n",
    "                sent_pointer += 1\n",
    "            else: #insert the word in\n",
    "                new_sent.append(edit)\n",
    "    if sent_pointer < len(sent):\n",
    "        for i in range(sent_pointer,len(sent)):\n",
    "            new_sent.append(sent[i])\n",
    "    return new_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sent1 = \"Military experts say the line between combat is getting blurry.\"\n",
    "#sent2 = \"Military experts say war is changing.\"\n",
    "#sent1 = \"According to Ledford, Northrop executives said they would build substantial parts of the bomber in Palmdale, creating about 1,500 jobs.\"\n",
    "#sent2 = \"According to Ledford, Northrop said they would build most of the bomber parts in Palmdale. It would create 1,500 jobs.\"\n",
    "#sent1 = \"Their eyes are quite small, and their visual acuity is poor.\"\n",
    "#sent2 = \"Their eyes are very small, and they do not see well.\"\n",
    "sent1 = \"In return, Rollo swore fealty to Charles, converted to Christianity, and undertook to defend the northern region of France against the Incursions of other Viking groups.\"\n",
    "sent2 = \"Rollo swore to be loyal to Charles, then he changed his religion to Christianity. Rollo protected northern France by fighting Viking invaders.\"\n",
    "sent1 = \"A fee is the price one pays as remuneration for services, especially the Honorarium paid to a doctor, lawyer, consultant, or other member of a learned profession.\"\n",
    "sent2 = \"A price one might pay for services is a called a fee.\"\n",
    "sent1 = \"Thereafter the county's administration was conducted at Duns or Lauder until Greenlaw became the county town in 1596.\"\n",
    "sent2 = \"After that, the county offices were at Duns or Lauder. In 1596 they moved to Greenlaw.\"\n",
    "sent1 = \"MacGruber starts asking for simple objects to make something to defuse the bomb, but he is later distracted by something (usually involving his personal life) that makes him run out of time.\"\n",
    "sent2 = \"Macgruber starts by asking for simple objects to stop the bomb from working. later he is distracted by an event from his personal life. as a result, he runs out of time to stop the bomb.\"\n",
    "sent1 = \"As the largest sub-region in Mesoamerica, it encompassed a vast and varied landscape, from the mountainous regions of the Sierra Madre to the semi-arid plains of northern Yucatán.\"\n",
    "sent2 = \"As the largest sub-region in Mesoamerica, it was a vast and varied landscape.\"\n",
    "sent1 = 'The tongue is sticky because of the presence of glycoprotein-rich mucous, which both lubricates movement in and out of the snout and helps to catch ants and termites, which adhere to it.'\n",
    "sent2 = 'The sticky tongue helps to catch bugs.'\n",
    "sent1 = 'The polymer is most often epoxy, but other polymers, such as polyester, vinyl Ester or nylon, are also sometimes used.'\n",
    "sent2 = 'The most popular polymer to use is epoxy.'\n",
    "sent1 = 'Together they formed New Music Manchester, a group committed to contemporary music.'\n",
    "sent2 = 'Both of the formed a group committed to contemporary music called new music Manchester.'\n",
    "sent1_tok = nltk.word_tokenize(re.sub(r'[\\(\\)\\`\\'\\\"]', '', sent1))\n",
    "sent2_tok = nltk.word_tokenize(re.sub(r'[\\(\\)\\`\\'\\\"]', '', sent2))\n",
    "\n",
    "A = edit_distance(sent1_tok, sent2_tok, max_id=4999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = \"The International fight League was an American mixed martial arts( Mma) promotion billed as the world's first Mma League.\"\n",
    "sent2 = \"The International fight League was billed as the world's first mixed martial arts (Mma) League.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = \"Aside from this, Cameron has often worked in Christian-Themed productions, among them the Post-Rapture films left behind: the movie, left behind II: tribulation force, and left behind: world at war, in which he plays Cameron `` Buck'' Williams.\"\n",
    "sent2 = 'Cameron has often worked in Christian-Themed productions, among them are left behind: the movie, left behind II: tribulation force, and left behind: world at war, in which he plays Cameron \"Buck\" Williams.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = 'Characteristics Radar observations indicate a fairly pure iron-nickel composition.'\n",
    "sent2 = 'A mainly pure Iron-Nickel composition was observed by radar.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1_tok = nltk.word_tokenize(re.sub(r'[\\(\\)\\`\\'\\\"]', '', sent1))\n",
    "sent2_tok = nltk.word_tokenize(re.sub(r'[\\(\\)\\`\\'\\\"]', '', sent2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['KEEP', 'KEEP', 'by', 'KEEP', 'KEEP', 'KEEP', 'KEEP', 'KEEP', 'stop', 'DEL', 'DEL', 'DEL', 'DEL', 'KEEP', 'KEEP', 'from', 'working', '.', 'later', 'DEL', 'DEL', 'KEEP', 'KEEP', 'DEL', 'KEEP', 'KEEP', 'an', 'event', 'from', 'DEL', 'DEL', 'DEL', 'DEL', 'KEEP', 'KEEP', 'KEEP', '.', 'as', 'a', 'result', ',', 'he', 'runs', 'DEL', 'DEL', 'DEL', 'DEL', 'DEL', 'KEEP', 'KEEP', 'KEEP', 'to', 'stop', 'the', 'bomb', 'KEEP']\n",
      "['MacGruber', 'starts', 'by', 'asking', 'for', 'simple', 'objects', 'to', 'stop', 'the', 'bomb', 'from', 'working', '.', 'later', 'he', 'is', 'distracted', 'by', 'an', 'event', 'from', 'his', 'personal', 'life', '.', 'as', 'a', 'result', ',', 'he', 'runs', 'out', 'of', 'time', 'to', 'stop', 'the', 'bomb', '.']\n"
     ]
    }
   ],
   "source": [
    "B = sent2edit(sent1_tok, sent2_tok)\n",
    "print(B)\n",
    "print(edit2sent(sent1_tok,B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ad_spans(edits):\n",
    "    ad_spans = []\n",
    "    seen_a = [0 for i in range(len(edits))]\n",
    "    for i in range(len(edits) - 1):\n",
    "        if seen_a[i] != 1:\n",
    "            if edits[i] != 'KEEP' and edits[i] != 'DEL':\n",
    "                start = i\n",
    "                j = i + 1\n",
    "                flag = False\n",
    "                while j < len(edits):\n",
    "                    if edits[j] == 'DEL':\n",
    "                        j += 1\n",
    "                        flag = True\n",
    "                    elif edits[j] != 'KEEP' and edits[j] != 'DEL':\n",
    "                        if flag == False:\n",
    "                            seen_a[j] = 1\n",
    "                            j += 1\n",
    "                        else:\n",
    "                            break\n",
    "                    else:\n",
    "                        break\n",
    "                if flag == True:\n",
    "                    end = j - 1\n",
    "                    if end - start > 0:\n",
    "                        ad_spans.append((start, end))\n",
    "    return ad_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_d_starts_from_ad_spans(edits, ad_spans):\n",
    "    d_starts = []\n",
    "    for span in ad_spans:\n",
    "        a_start = span[0]\n",
    "        d_start = a_start\n",
    "        while d_start < len(edits):\n",
    "            if edits[d_start] != 'DEL':\n",
    "                d_start += 1\n",
    "            else:\n",
    "                break\n",
    "        d_starts.append(d_start)\n",
    "    return d_starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8, 12), (15, 20), (26, 32), (36, 47)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_ad_spans(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 19, 29, 43]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_d_starts_from_ad_spans(B, extract_ad_spans(B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = \"MacGruber starts asking for simple objects to make something to defuse the bomb, but he is later distracted by something (usually involving his personal life) that makes him run out of time.\"\n",
    "sent2 = \"Macgruber starts by asking for simple objects to stop the bomb from working. later he is distracted by an event from his personal life. as a result, he runs out of time to stop the bomb.\"\n",
    "sent1_tok = nltk.word_tokenize(re.sub(r'[\\(\\)\\`\\'\\\"]', '', sent1))\n",
    "sent2_tok = nltk.word_tokenize(re.sub(r'[\\(\\)\\`\\'\\\"]', '', sent2))\n",
    "edits = sent2edit(sent1_tok, sent2_tok)\n",
    "ad_spans = extract_ad_spans(edits)\n",
    "aligns = [([21, 22], [21]), ([0], [0]), ([1], [1, 2]), ([2], [3]), ([3], [4]), ([4], [5]), ([5], [6]), ([6], [7]), ([10], [8]), ([11], [37]), ([12], [38]), ([13], [13]), ([15], [15]), ([16], [16]), ([17], [14]), ([18], [17]), ([19], [18]), ([20], [19, 20]), ([23], [22]), ([24], [23]), ([25], [24]), ([28], [30]), ([29], [31]), ([30], [32]), ([31], [33]), ([32], [34]), ([33], [39])] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MacGruber', 'starts', 'asking', 'for', 'simple', 'objects', 'to', 'make', 'something', 'to', 'defuse', 'the', 'bomb', ',', 'but', 'he', 'is', 'later', 'distracted', 'by', 'something', 'usually', 'involving', 'his', 'personal', 'life', 'that', 'makes', 'him', 'run', 'out', 'of', 'time', '.']\n",
      "['Macgruber', 'starts', 'by', 'asking', 'for', 'simple', 'objects', 'to', 'stop', 'the', 'bomb', 'from', 'working', '.', 'later', 'he', 'is', 'distracted', 'by', 'an', 'event', 'from', 'his', 'personal', 'life', '.', 'as', 'a', 'result', ',', 'he', 'runs', 'out', 'of', 'time', 'to', 'stop', 'the', 'bomb', '.']\n"
     ]
    }
   ],
   "source": [
    "print(sent1_tok)\n",
    "print(sent2_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_spans = extract_ad_spans(edits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_ids_to_edits(edits, sent2_tok):\n",
    "    edits_ids = []\n",
    "    sent1_pointer = 0\n",
    "    sent2_pointer = 0\n",
    "    for i in range(len(edits)):\n",
    "        if edits[i] == 'KEEP':\n",
    "            edits_ids.append(sent1_pointer)\n",
    "            sent1_pointer += 1\n",
    "        elif edits[i] == 'DEL':\n",
    "            edits_ids.append(sent1_pointer)\n",
    "            sent1_pointer += 1\n",
    "        else:\n",
    "            while sent2_pointer < len(sent2_tok):\n",
    "                if sent2_tok[sent2_pointer] == edits[i]:\n",
    "                    edits_ids.append(sent2_pointer)\n",
    "                    sent2_pointer += 1\n",
    "                    break\n",
    "                else:\n",
    "                    sent2_pointer += 1\n",
    "    return edits_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['KEEP', 'KEEP', 'by', 'KEEP', 'KEEP', 'KEEP', 'KEEP', 'KEEP', 'stop', 'DEL', 'DEL', 'DEL', 'DEL', 'KEEP', 'KEEP', 'from', 'working', '.', 'later', 'DEL', 'DEL', 'KEEP', 'KEEP', 'DEL', 'KEEP', 'KEEP', 'an', 'event', 'from', 'DEL', 'DEL', 'DEL', 'KEEP', 'KEEP', 'KEEP', '.', 'as', 'a', 'result', ',', 'he', 'runs', 'DEL', 'DEL', 'DEL', 'DEL', 'KEEP', 'KEEP', 'KEEP', 'to', 'stop', 'the', 'bomb', 'KEEP']\n",
      "[0, 1, 2, 2, 3, 4, 5, 6, 8, 7, 8, 9, 10, 11, 12, 11, 12, 13, 14, 13, 14, 15, 16, 17, 18, 19, 19, 20, 21, 20, 21, 22, 23, 24, 25, 25, 26, 27, 28, 29, 30, 31, 26, 27, 28, 29, 30, 31, 32, 35, 36, 37, 38, 33]\n"
     ]
    }
   ],
   "source": [
    "print(edits)\n",
    "print(assign_ids_to_edits(edits, sent2_tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = \"MacGruber starts asking for simple objects to make something to defuse the bomb, but he is later distracted by something (usually involving his personal life) that makes him run out of time.\"\n",
    "sent2 = \"Macgruber starts by asking for simple objects to stop the bomb from working. later he is distracted by an event from his personal life. as a result, he runs out of time to stop the bomb.\"\n",
    "sent1_tok = nltk.word_tokenize(re.sub(r'[\\(\\)\\`\\'\\\"]', '', sent1))\n",
    "sent2_tok = nltk.word_tokenize(re.sub(r'[\\(\\)\\`\\'\\\"]', '', sent2))\n",
    "edits = sent2edit(sent1_tok, sent2_tok)\n",
    "ad_spans = extract_ad_spans(edits)\n",
    "aligns = [([21, 22], [21]), ([0], [0]), ([1], [1, 2]), ([2], [3]), ([3], [4]), ([4], [5]), ([5], [6]), ([6], [7]), ([10], [8]), ([11], [37]), ([12], [38]), ([13], [13]), ([15], [15]), ([16], [16]), ([17], [14]), ([18], [17]), ([19], [18]), ([20], [19, 20]), ([23], [22]), ([24], [23]), ([25], [24]), ([28], [30]), ([29], [31]), ([30], [32]), ([31], [33]), ([32], [34]), ([33], [39])] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n",
      "54\n"
     ]
    }
   ],
   "source": [
    "print(len(edits))\n",
    "print(len(assign_ids_to_edits(edits, sent2_tok)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8, 12), (15, 20), (26, 31), (35, 45)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ad_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_splr_ids(edits, ad_spans, sent2_tok):\n",
    "    edits_ids = assign_ids_to_edits(edits, sent2_tok)\n",
    "    splr_ids = []\n",
    "    for ad_span_idx in range(len(ad_spans)):\n",
    "        splr_flag = False\n",
    "        for i in range(ad_spans[ad_span_idx][0], ad_spans[ad_span_idx][1] + 1):\n",
    "            if edits[i] == '.':\n",
    "                splr_flag = True\n",
    "        if splr_flag == True:\n",
    "            #sent1_span = [edits_ids[j] for j in range(ad_spans[ad_span_idx][0], ad_spans[ad_span_idx][1]+1) if edits[j] == 'KEEP' or edits[j] == 'DEL']\n",
    "            #sent2_span = [edits_ids[j] for j in range(ad_spans[ad_span_idx][0], ad_spans[ad_span_idx][1]+1) if edits[j] != 'KEEP' and edits[j] != 'DEL']\n",
    "            #splr_ids.append((sent1_span, sent2_span, ad_span_idx))\n",
    "            splr_ids.append(['splr_span', ad_span_idx])\n",
    "    return splr_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['splr_span', 1], ['splr_span', 3]]\n"
     ]
    }
   ],
   "source": [
    "splr_ids = extract_splr_ids(edits, ad_spans, sent2_tok)\n",
    "print(splr_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3]\n"
     ]
    }
   ],
   "source": [
    "ad_spans_done = [i[1] for i in splr_ids]\n",
    "print(ad_spans_done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8, 12), (15, 20), (26, 31), (35, 45)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ad_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 19, 29, 42]\n"
     ]
    }
   ],
   "source": [
    "d_starts = extract_d_starts_from_ad_spans(edits, ad_spans)\n",
    "print(d_starts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['to', 'defuse', 'the', 'bomb']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent1_tok[9:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stop', 'DEL', 'DEL', 'DEL', 'DEL']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edits[8:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = \"MacGruber starts asking for simple objects to make something to defuse the bomb, but he is later distracted by something (usually involving his personal life) that makes him run out of time.\"\n",
    "sent2 = \"Macgruber starts by asking for simple objects to stop the bomb from working. later he is distracted by an event from his personal life. as a result, he runs out of time to stop the bomb.\"\n",
    "sent1_tok = nltk.word_tokenize(re.sub(r'[\\(\\)\\`\\'\\\"]', '', sent1))\n",
    "sent2_tok = nltk.word_tokenize(re.sub(r'[\\(\\)\\`\\'\\\"]', '', sent2))\n",
    "edits = sent2edit(sent1_tok, sent2_tok)\n",
    "ad_spans = extract_ad_spans(edits)\n",
    "aligns = [([21, 22], [21]), ([0], [0]), ([1], [1, 2]), ([2], [3]), ([3], [4]), ([4], [5]), ([5], [6]), ([6], [7]), ([10], [8]), ([11], [37]), ([12], [38]), ([13], [13]), ([15], [15]), ([16], [16]), ([17], [14]), ([18], [17]), ([19], [18]), ([20], [19, 20]), ([23], [22]), ([24], [23]), ([25], [24]), ([28], [30]), ([29], [31]), ([30], [32]), ([31], [33]), ([32], [34]), ([33], [39])] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_rep_ids(edits, ad_spans, sent2_tok, aligns, splr_ids):\n",
    "    edits_ids = assign_ids_to_edits(edits, sent2_tok)\n",
    "    d_starts = extract_d_starts_from_ad_spans(edits, ad_spans)\n",
    "    rep_ids = []\n",
    "    ad_spans_done = [i[1] for i in splr_ids]\n",
    "    for ad_span_idx in range(len(ad_spans)):\n",
    "        if ad_span_idx in ad_spans_done:\n",
    "            continue\n",
    "        else:\n",
    "            now_span = ad_spans[ad_span_idx]\n",
    "            d_start_in_now_span = d_starts[ad_span_idx]\n",
    "            d_span_in_now_span = list(range(d_start_in_now_span, now_span[1]+1))\n",
    "            a_span_in_now_span = list(range(now_span[0], d_start_in_now_span))\n",
    "            sent1_ids_corresponding_d_span_in_now_span = [edits_ids[i] for i in d_span_in_now_span if edits[i] == 'DEL']\n",
    "            sent2_ids_corresponding_a_span_in_now_span = [edits_ids[i] for i in a_span_in_now_span if edits[i] != 'DEL']\n",
    "\n",
    "            added_words_to_sent1_by_a_span = []\n",
    "            for i in range(len(sent2_ids_corresponding_a_span_in_now_span)):\n",
    "                added_words_to_sent1_by_a_span.append(sent2_tok[sent2_ids_corresponding_a_span_in_now_span[i]])\n",
    "\n",
    "            aligned_words_in_sent2 = []\n",
    "            for i in range(len(sent1_ids_corresponding_d_span_in_now_span)):\n",
    "                for j in range(len(aligns)):\n",
    "                    if sent1_ids_corresponding_d_span_in_now_span[i] in aligns[j][0]:\n",
    "                        for aligned_word_id in aligns[j][1]:\n",
    "                            aligned_words_in_sent2.append(sent2_tok[aligned_word_id])\n",
    "\n",
    "            rep_flag = False\n",
    "            for word in aligned_words_in_sent2:\n",
    "                if word in added_words_to_sent1_by_a_span:\n",
    "                    rep_flag = True\n",
    "            \n",
    "            if rep_flag == True:\n",
    "                rep_ids.append(['rep_span', ad_span_idx])\n",
    "    \n",
    "    return rep_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['rep_span', 0], ['rep_span', 2]]\n"
     ]
    }
   ],
   "source": [
    "rep_ids = extract_rep_ids(edits, ad_spans, sent2_tok, aligns, ad_spans_done)\n",
    "print(rep_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['make', 'something', 'to', 'defuse'] ['stop']\n"
     ]
    }
   ],
   "source": [
    "print(sent1_tok[7:11], sent2_tok[8:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['something', 'usually', 'involving'] ['an', 'event', 'from']\n"
     ]
    }
   ],
   "source": [
    "print(sent1_tok[20:23], sent2_tok[19:22])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['an', 'event', 'from', 'from']\n"
     ]
    }
   ],
   "source": [
    "test_d_span = [9, 10, 11, 12]\n",
    "test_d_span = [29, 30, 31]\n",
    "edits_ids = assign_ids_to_edits(edits, sent2_tok)\n",
    "sent1_span = [edits_ids[i] for i in test_d_span if edits[i] == 'DEL']\n",
    "aligned_words_in_sent2 = []\n",
    "for i in range(len(sent1_span)):\n",
    "    for j in range(len(aligns)):\n",
    "        if sent1_span[i] in aligns[j][0]:\n",
    "            for aligned_word_id in aligns[j][1]:\n",
    "                aligned_words_in_sent2.append(sent2_tok[aligned_word_id])\n",
    "print(aligned_words_in_sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = 'Characteristics Radar observations indicate a fairly pure iron-nickel composition.'\n",
    "sent2 = 'A mainly pure Iron-Nickel composition was observed by radar.'\n",
    "sent1_tok = nltk.word_tokenize(re.sub(r'[\\(\\)\\`\\'\\\"]', '', sent1))\n",
    "sent2_tok = nltk.word_tokenize(re.sub(r'[\\(\\)\\`\\'\\\"]', '', sent2))\n",
    "edits = sent2edit(sent1_tok, sent2_tok)\n",
    "ad_spans = extract_ad_spans(edits)\n",
    "aligns = [([1, 2], [8]), ([3], [6, 7]), ([4], [0]), ([5], [1]), ([6], [2]), ([7], [3]), ([8], [4]), ([9], [9])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DEL', 'DEL', 'DEL', 'DEL', 'KEEP', 'mainly', 'DEL', 'KEEP', 'KEEP', 'KEEP', 'was', 'observed', 'by', 'radar', 'KEEP']\n",
      "[(5, 6)]\n"
     ]
    }
   ],
   "source": [
    "print(edits)\n",
    "print(ad_spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_d_spans(edits):\n",
    "    d_spans = []\n",
    "    flag = False\n",
    "    seen = [0 for i in range(len(edits))]\n",
    "    for i in range(len(edits)):\n",
    "        if seen[i] != 1:\n",
    "            seen[i] = 1\n",
    "            if edits[i] != 'KEEP' and edits[i] != 'DEL':\n",
    "                flag = True\n",
    "            elif edits[i] == 'KEEP':\n",
    "                flag = False\n",
    "            else:\n",
    "                if flag == True:\n",
    "                    continue\n",
    "                else:\n",
    "                    start = i\n",
    "                    j = i + 1\n",
    "                    while j < len(edits):\n",
    "                        if edits[j] == 'DEL':\n",
    "                            seen[j] = 1\n",
    "                            j += 1\n",
    "                        elif edits[j] == 'KEEP':\n",
    "                            break\n",
    "                        else:\n",
    "                            flag = True\n",
    "                            break\n",
    "                    end = j - 1\n",
    "                    if end - start >= 0:\n",
    "                        d_spans.append((start, end))\n",
    "    return d_spans        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 3)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_d_spans(edits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_a_spans(edits):\n",
    "    a_spans = []\n",
    "    seen_a = [0 for i in range(len(edits))]\n",
    "    for i in range(len(edits)):\n",
    "        if seen_a[i] != 1:\n",
    "            seen_a[i] = 1\n",
    "            if edits[i] != 'KEEP' and edits[i] != 'DEL':\n",
    "                start = i\n",
    "                j = i + 1\n",
    "                while j < len(edits):\n",
    "                    flag = False\n",
    "                    if edits[j] != 'KEEP' and edits[j] != 'DEL':\n",
    "                        seen_a[j] = 1\n",
    "                        j += 1\n",
    "                    elif edits[j] == 'DEL':\n",
    "                        flag = True\n",
    "                        break\n",
    "                    else:\n",
    "                        break\n",
    "                end = j - 1\n",
    "                if (flag == False) and (end - start >= 0):\n",
    "                    a_spans.append((start, end))\n",
    "    return a_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(10, 13)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_a_spans(edits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mvr_ids(edits, ad_spans, d_spans, a_spans, sent2_tok, aligns):\n",
    "    edits_ids = assign_ids_to_edits(edits, sent2_tok)\n",
    "    mvr_ids = []\n",
    "    seen_a_span = [0 for i in range(len(a_spans))]\n",
    "    seen_ad_span = [0 for i in range(len(ad_spans))]\n",
    "    for d_span_idx in range(len(d_spans)):\n",
    "        tmp_ids = ['mvr_span']\n",
    "        d_span = list(range(d_spans[d_span_idx][0], d_spans[d_span_idx][1]+1))\n",
    "        sent1_ids_corresponding_d_span = [edits_ids[i] for i in d_span if edits[i] == 'DEL']\n",
    "\n",
    "        aligned_words_in_sent2_from_d_span = []\n",
    "        for i in range(len(sent1_ids_corresponding_d_span)):\n",
    "            for j in range(len(aligns)):\n",
    "                if sent1_ids_corresponding_d_span[i] in aligns[j][0]:\n",
    "                    for aligned_word_id in aligns[j][1]:\n",
    "                        aligned_words_in_sent2_from_d_span.append(sent2_tok[aligned_word_id])\n",
    "        \n",
    "        if len(aligned_words_in_sent2_from_d_span) == 0:\n",
    "            continue\n",
    "        else:\n",
    "            tmp_ids.append(('d_span', d_span_idx))\n",
    "\n",
    "            # check a_spans\n",
    "            for a_span_idx in range(len(a_spans)):\n",
    "                a_span = list(range(a_spans[a_span_idx][0], a_spans[a_span_idx][1]+1))\n",
    "                if d_span[-1] > a_span[0]:\n",
    "                    continue\n",
    "                else:\n",
    "                    sent2_ids_corresponding_a_span = [edits_ids[i] for i in a_span if edits[i] != 'DEL']\n",
    "                    added_words_to_sent1_by_a_span = []\n",
    "                    for i in range(len(sent2_ids_corresponding_a_span)):\n",
    "                        added_words_to_sent1_by_a_span.append(sent2_tok[sent2_ids_corresponding_a_span[i]])\n",
    "                    \n",
    "                    flag = False\n",
    "                    for word in aligned_words_in_sent2_from_d_span:\n",
    "                        if word in added_words_to_sent1_by_a_span:\n",
    "                            flag = True\n",
    "                    if flag == True:\n",
    "                        if seen_a_span[a_span_idx] == 0:\n",
    "                            tmp_ids.append(('a_span', a_span_idx))\n",
    "                            seen_a_span[a_span_idx] = 1\n",
    "            \n",
    "            # check ad_span\n",
    "            d_starts = extract_d_starts_from_ad_spans(edits, ad_spans)\n",
    "            for ad_span_idx in range(len(ad_spans)):\n",
    "                ad_span = list(range(ad_spans[ad_span_idx][0], ad_spans[ad_span_idx][1]+1))\n",
    "                if d_span[-1] > ad_span[0]:\n",
    "                    continue\n",
    "                else:\n",
    "                    d_start_in_ad_span = d_starts[ad_span_idx]\n",
    "                    d_span_in_ad_span = list(range(d_start_in_ad_span, ad_span[-1]+1))\n",
    "                    a_span_in_ad_span = list(range(ad_span[0], d_start_in_ad_span))\n",
    "                    sent1_ids_corresponding_d_span_in_ad_span = [edits_ids[i] for i in d_span_in_ad_span if edits[i] == 'DEL']\n",
    "                    sent2_ids_corresponding_a_span_in_ad_span = [edits_ids[i] for i in a_span_in_ad_span if edits[i] != 'DEL']\n",
    "                    \n",
    "                    added_words_to_sent1_by_a_span_in_ad_span = []\n",
    "                    for i in range(len(sent2_ids_corresponding_a_span_in_ad_span)):\n",
    "                        added_words_to_sent1_by_a_span_in_ad_span.append(sent2_tok[sent2_ids_corresponding_a_span_in_ad_span[i]])\n",
    "\n",
    "                    aligned_words_in_sent2_from_ad_span = []\n",
    "                    for i in range(len(sent1_ids_corresponding_d_span_in_ad_span)):\n",
    "                        for j in range(len(aligns)):\n",
    "                            if sent1_ids_corresponding_d_span_in_ad_span[i] in aligns[j][0]:\n",
    "                                for aligned_word_id in aligns[j][1]:\n",
    "                                    aligned_words_in_sent2_from_ad_span.append(sent2_tok[aligned_word_id])\n",
    "                    \n",
    "                    flag = False\n",
    "                    if len(aligned_words_in_sent2_from_ad_span) == 0:\n",
    "                        flag = True\n",
    "                    for word in aligned_words_in_sent2_from_d_span:\n",
    "                        if word in added_words_to_sent1_by_a_span_in_ad_span:\n",
    "                            flag = True\n",
    "                    if flag == True:\n",
    "                        if seen_ad_span[ad_span_idx] == 0:\n",
    "                            tmp_ids.append(('ad_span', ad_span_idx))\n",
    "                            seen_ad_span[ad_span_idx] = 1\n",
    "\n",
    "            mvr_ids.append(tmp_ids)\n",
    "    return mvr_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_d_ids(edits, d_spans, sent2_tok, aligns):\n",
    "    edits_ids = assign_ids_to_edits(edits, sent2_tok)\n",
    "    d_ids = []\n",
    "    for d_span_idx in range(len(d_spans)):\n",
    "        d_span = list(range(d_spans[d_span_idx][0], d_spans[d_span_idx][1]+1))\n",
    "        sent1_ids_corresponding_d_span = [edits_ids[i] for i in d_span if edits[i] == 'DEL']\n",
    "\n",
    "        aligned_words_in_sent2_form_d_span = []\n",
    "        for i in range(len(sent1_ids_corresponding_d_span)):\n",
    "            for j in range(len(aligns)):\n",
    "                if sent1_ids_corresponding_d_span[i] in aligns[j][0]:\n",
    "                    for aligned_word_id in aligns[j][1]:\n",
    "                        aligned_words_in_sent2_form_d_span.append(sent2_tok[aligned_word_id])\n",
    "        \n",
    "        if len(aligned_words_in_sent2_form_d_span) == 0:\n",
    "            d_ids.append(['d_span', d_span_idx])\n",
    "    \n",
    "    return d_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_a_ids(a_spans, mvr_ids):\n",
    "    a_ids = []\n",
    "    a_spans_done = []\n",
    "    for mvr_info in mvr_ids:\n",
    "        for i in range(1, len(mvr_info)):\n",
    "            if mvr_info[i][0] == 'a_span':\n",
    "                a_spans_done.append(mvr_info[i][1])\n",
    "\n",
    "    for a_span_idx in range(len(a_spans)):\n",
    "        if a_span_idx in a_spans_done:\n",
    "            continue\n",
    "        else:\n",
    "            a_ids.append(['a_span', a_span_idx])\n",
    "\n",
    "    return a_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A [fee is the] price one pays as remuneration for services [(is) , especially the Honorarium paid to] a [(called) doctor, lawyer, consultant, or other member of] a [(fee) Kearney profession] .  \n",
    "  \n",
    "  \n",
    "A [] price one pays as remuneration for services [(is) ] a [(called) ] a [(fee) ] .  \n",
    "  \n",
    "\n",
    "move-rephrase spanは、alignmentありd_spanと、それより後ろにある、「条件」を満たしたa_span/ad_spanをまとめたもの。  \n",
    "条件（a_span）：今見ているd_spanのalignment先の単語を含んでいる\n",
    "条件（ad_span）：今見ているd_spanのalignment先の単語を含んでいる、またはad_span中のd_spanが1つもalignmentを持たない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = 'Characteristics Radar observations indicate a fairly pure iron-nickel composition.'\n",
    "sent2 = 'A mainly pure Iron-Nickel composition was observed by radar.'\n",
    "sent1_tok = nltk.word_tokenize(re.sub(r'[\\(\\)\\`\\'\\\"]', '', sent1))\n",
    "sent2_tok = nltk.word_tokenize(re.sub(r'[\\(\\)\\`\\'\\\"]', '', sent2))\n",
    "edits = sent2edit(sent1_tok, sent2_tok)\n",
    "ad_spans = extract_ad_spans(edits)\n",
    "d_spans = extract_d_spans(edits)\n",
    "a_spans = extract_a_spans(edits)\n",
    "aligns = [([1, 2], [8]), ([3], [6, 7]), ([4], [0]), ([5], [1]), ([6], [2]), ([7], [3]), ([8], [4]), ([9], [9])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = \"MacGruber starts asking for simple objects to make something to defuse the bomb, but he is later distracted by something (usually involving his personal life) that makes him run out of time.\"\n",
    "sent2 = \"Macgruber starts by asking for simple objects to stop the bomb from working. later he is distracted by an event from his personal life. as a result, he runs out of time to stop the bomb.\"\n",
    "sent1_tok = nltk.word_tokenize(re.sub(r'[\\(\\)\\`\\'\\\"]', '', sent1))\n",
    "sent2_tok = nltk.word_tokenize(re.sub(r'[\\(\\)\\`\\'\\\"]', '', sent2))\n",
    "edits = sent2edit(sent1_tok, sent2_tok)\n",
    "ad_spans = extract_ad_spans(edits)\n",
    "d_spans = extract_d_spans(edits)\n",
    "a_spans = extract_a_spans(edits)\n",
    "aligns = [([21, 22], [21]), ([0], [0]), ([1], [1, 2]), ([2], [3]), ([3], [4]), ([4], [5]), ([5], [6]), ([6], [7]), ([10], [8]), ([11], [37]), ([12], [38]), ([13], [13]), ([15], [15]), ([16], [16]), ([17], [14]), ([18], [17]), ([19], [18]), ([20], [19, 20]), ([23], [22]), ([24], [23]), ([25], [24]), ([28], [30]), ([29], [31]), ([30], [32]), ([31], [33]), ([32], [34]), ([33], [39])] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8, 12), (15, 20), (26, 31), (35, 45)]\n"
     ]
    }
   ],
   "source": [
    "print(ad_spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_to_edit(edits, sent1_tok, sent2_tok):\n",
    "    text_to_edit = []\n",
    "    edits_ids = assign_ids_to_edits(edits, sent2_tok)\n",
    "\n",
    "    doc_sent1 = nlp(\" \".join(sent1_tok))\n",
    "    sent1_tok_pos_lemma = []\n",
    "    for i, token in enumerate(doc_sent1):\n",
    "        sent1_tok_pos_lemma.append([sent1_tok[i], token.pos_, token.lemma_])\n",
    "\n",
    "    doc_sent2 = nlp(\" \".join(sent2_tok))\n",
    "    sent2_tok_pos_lemma = []\n",
    "    for i, token in enumerate(doc_sent2):\n",
    "        sent2_tok_pos_lemma.append([sent2_tok[i], token.pos_, token.lemma_])\n",
    " \n",
    "    for i in range(len(edits)):\n",
    "        if edits[i] == 'DEL' or edits[i] == 'KEEP':\n",
    "            text_to_edit.append([sent1_tok_pos_lemma[edits_ids[i]][0], sent1_tok_pos_lemma[edits_ids[i]][1], sent1_tok_pos_lemma[edits_ids[i]][2]])\n",
    "        else:\n",
    "            word_in_sent2 = sent2_tok_pos_lemma[edits_ids[i]][0]\n",
    "            word_in_sent2 = 'ADD_' + word_in_sent2\n",
    "            text_to_edit.append([word_in_sent2, sent2_tok_pos_lemma[edits_ids[i]][1], sent2_tok_pos_lemma[edits_ids[i]][2]])\n",
    "    return text_to_edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['MacGruber', 'PROPN', 'MacGruber'], ['starts', 'VERB', 'start'], ['ADD_by', 'ADP', 'by'], ['asking', 'VERB', 'ask'], ['for', 'ADP', 'for'], ['simple', 'ADJ', 'simple'], ['objects', 'NOUN', 'object'], ['to', 'PART', 'to'], ['ADD_stop', 'VERB', 'stop'], ['make', 'VERB', 'make'], ['something', 'PRON', 'something'], ['to', 'PART', 'to'], ['defuse', 'VERB', 'defuse'], ['the', 'DET', 'the'], ['bomb', 'NOUN', 'bomb'], ['ADD_from', 'ADP', 'from'], ['ADD_working', 'VERB', 'work'], ['ADD_.', 'PUNCT', '.'], ['ADD_later', 'ADV', 'later'], [',', 'PUNCT', ','], ['but', 'CCONJ', 'but'], ['he', 'PRON', 'he'], ['is', 'AUX', 'be'], ['later', 'ADV', 'later'], ['distracted', 'VERB', 'distract'], ['by', 'ADP', 'by'], ['ADD_an', 'DET', 'an'], ['ADD_event', 'NOUN', 'event'], ['ADD_from', 'ADP', 'from'], ['something', 'PRON', 'something'], ['usually', 'ADV', 'usually'], ['involving', 'VERB', 'involve'], ['his', 'PRON', 'his'], ['personal', 'ADJ', 'personal'], ['life', 'NOUN', 'life'], ['ADD_.', 'PUNCT', '.'], ['ADD_as', 'ADP', 'as'], ['ADD_a', 'DET', 'a'], ['ADD_result', 'NOUN', 'result'], ['ADD_,', 'PUNCT', ','], ['ADD_he', 'PRON', 'he'], ['ADD_runs', 'VERB', 'run'], ['that', 'PRON', 'that'], ['makes', 'VERB', 'make'], ['him', 'PRON', 'he'], ['run', 'VERB', 'run'], ['out', 'ADP', 'out'], ['of', 'ADP', 'of'], ['time', 'NOUN', 'time'], ['ADD_to', 'PART', 'to'], ['ADD_stop', 'VERB', 'stop'], ['ADD_the', 'DET', 'the'], ['ADD_bomb', 'NOUN', 'bomb'], ['.', 'PUNCT', '.']]\n"
     ]
    }
   ],
   "source": [
    "text_to_edit = create_text_to_edit(edits, sent1_tok, sent2_tok)\n",
    "print(text_to_edit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "by\n"
     ]
    }
   ],
   "source": [
    "print(text_to_edit[2][0][4:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_height(root):\n",
    "    if not list(root.children):\n",
    "        return 1\n",
    "    else:\n",
    "        return 1 + max(tree_height(x) for x in root.children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_max_deptree_depth(str_text):\n",
    "    doc = nlp(str_text)\n",
    "    roots = [sent.root for sent in doc.sents]\n",
    "    return max([tree_height(root) for root in roots])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "freq_data = pd.read_csv('./src/lemmas_60k_m2166.txt', skiprows=8, header=0, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.29350711581128"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.log(freq_data[(freq_data['PoS'] == 'j') | (freq_data['PoS'] == 'r') | (freq_data['PoS'] == 'n') | (freq_data['PoS'] == 'v')]['freq'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>lemma</th>\n",
       "      <th>PoS</th>\n",
       "      <th>freq</th>\n",
       "      <th>perMil</th>\n",
       "      <th>%caps</th>\n",
       "      <th>%allC</th>\n",
       "      <th>range</th>\n",
       "      <th>disp</th>\n",
       "      <th>blog</th>\n",
       "      <th>...</th>\n",
       "      <th>news</th>\n",
       "      <th>acad</th>\n",
       "      <th>blogPM</th>\n",
       "      <th>webPM</th>\n",
       "      <th>TVMPM</th>\n",
       "      <th>spokPM</th>\n",
       "      <th>ficPM</th>\n",
       "      <th>magPM</th>\n",
       "      <th>newsPM</th>\n",
       "      <th>acadPM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>the</td>\n",
       "      <td>a</td>\n",
       "      <td>50033612</td>\n",
       "      <td>50385.16</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.00</td>\n",
       "      <td>482995</td>\n",
       "      <td>0.98</td>\n",
       "      <td>6266654</td>\n",
       "      <td>...</td>\n",
       "      <td>6579270</td>\n",
       "      <td>7440931</td>\n",
       "      <td>50434.35</td>\n",
       "      <td>55169.32</td>\n",
       "      <td>29524.14</td>\n",
       "      <td>45672.67</td>\n",
       "      <td>53310.74</td>\n",
       "      <td>53941.86</td>\n",
       "      <td>54042.73</td>\n",
       "      <td>62116.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>be</td>\n",
       "      <td>v</td>\n",
       "      <td>32394756</td>\n",
       "      <td>32622.71</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>481177</td>\n",
       "      <td>0.99</td>\n",
       "      <td>5594001</td>\n",
       "      <td>...</td>\n",
       "      <td>4102481</td>\n",
       "      <td>3689062</td>\n",
       "      <td>45020.81</td>\n",
       "      <td>41409.63</td>\n",
       "      <td>62695.87</td>\n",
       "      <td>55701.50</td>\n",
       "      <td>37289.40</td>\n",
       "      <td>33259.44</td>\n",
       "      <td>33698.16</td>\n",
       "      <td>30795.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>and</td>\n",
       "      <td>c</td>\n",
       "      <td>24778098</td>\n",
       "      <td>24952.20</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>478670</td>\n",
       "      <td>0.98</td>\n",
       "      <td>3205178</td>\n",
       "      <td>...</td>\n",
       "      <td>2993061</td>\n",
       "      <td>3627686</td>\n",
       "      <td>25795.44</td>\n",
       "      <td>26849.01</td>\n",
       "      <td>14249.23</td>\n",
       "      <td>26275.16</td>\n",
       "      <td>25860.15</td>\n",
       "      <td>26177.63</td>\n",
       "      <td>24585.28</td>\n",
       "      <td>30283.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>24225478</td>\n",
       "      <td>24395.69</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>478204</td>\n",
       "      <td>0.99</td>\n",
       "      <td>3098338</td>\n",
       "      <td>...</td>\n",
       "      <td>3299770</td>\n",
       "      <td>2602697</td>\n",
       "      <td>24935.58</td>\n",
       "      <td>24747.22</td>\n",
       "      <td>20690.47</td>\n",
       "      <td>23374.72</td>\n",
       "      <td>25107.33</td>\n",
       "      <td>27541.93</td>\n",
       "      <td>27104.62</td>\n",
       "      <td>21727.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>of</td>\n",
       "      <td>i</td>\n",
       "      <td>23159162</td>\n",
       "      <td>23321.89</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>477933</td>\n",
       "      <td>0.97</td>\n",
       "      <td>2897295</td>\n",
       "      <td>...</td>\n",
       "      <td>2867922</td>\n",
       "      <td>4500485</td>\n",
       "      <td>23317.58</td>\n",
       "      <td>26588.36</td>\n",
       "      <td>10918.77</td>\n",
       "      <td>20157.43</td>\n",
       "      <td>19270.82</td>\n",
       "      <td>25796.39</td>\n",
       "      <td>23557.38</td>\n",
       "      <td>37569.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60995</th>\n",
       "      <td>60996</td>\n",
       "      <td>hand-embroidered</td>\n",
       "      <td>j</td>\n",
       "      <td>42</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>41</td>\n",
       "      <td>0.76</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60996</th>\n",
       "      <td>60997</td>\n",
       "      <td>wast</td>\n",
       "      <td>n</td>\n",
       "      <td>42</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>32</td>\n",
       "      <td>0.63</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60997</th>\n",
       "      <td>60998</td>\n",
       "      <td>watchband</td>\n",
       "      <td>n</td>\n",
       "      <td>42</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>38</td>\n",
       "      <td>0.73</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60998</th>\n",
       "      <td>60999</td>\n",
       "      <td>weak-side</td>\n",
       "      <td>j</td>\n",
       "      <td>42</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>39</td>\n",
       "      <td>0.70</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60999</th>\n",
       "      <td>61000</td>\n",
       "      <td>water-powered</td>\n",
       "      <td>j</td>\n",
       "      <td>42</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.00</td>\n",
       "      <td>33</td>\n",
       "      <td>0.61</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61000 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        rank             lemma PoS      freq    perMil  %caps  %allC   range  \\\n",
       "0          1               the   a  50033612  50385.16   0.11   0.00  482995   \n",
       "1          2                be   v  32394756  32622.71   0.03   0.01  481177   \n",
       "2          3               and   c  24778098  24952.20   0.09   0.00  478670   \n",
       "3          4                 a   a  24225478  24395.69   0.04   0.04  478204   \n",
       "4          5                of   i  23159162  23321.89   0.01   0.00  477933   \n",
       "...      ...               ...  ..       ...       ...    ...    ...     ...   \n",
       "60995  60996  hand-embroidered   j        42      0.04   0.00   0.00      41   \n",
       "60996  60997              wast   n        42      0.04   0.02   0.00      32   \n",
       "60997  60998         watchband   n        42      0.04   0.02   0.02      38   \n",
       "60998  60999         weak-side   j        42      0.04   0.07   0.00      39   \n",
       "60999  61000     water-powered   j        42      0.04   0.14   0.00      33   \n",
       "\n",
       "       disp     blog  ...     news     acad    blogPM     webPM     TVMPM  \\\n",
       "0      0.98  6266654  ...  6579270  7440931  50434.35  55169.32  29524.14   \n",
       "1      0.99  5594001  ...  4102481  3689062  45020.81  41409.63  62695.87   \n",
       "2      0.98  3205178  ...  2993061  3627686  25795.44  26849.01  14249.23   \n",
       "3      0.99  3098338  ...  3299770  2602697  24935.58  24747.22  20690.47   \n",
       "4      0.97  2897295  ...  2867922  4500485  23317.58  26588.36  10918.77   \n",
       "...     ...      ...  ...      ...      ...       ...       ...       ...   \n",
       "60995  0.76        1  ...        9        0      0.01      0.02      0.03   \n",
       "60996  0.63       11  ...        2        3      0.09      0.11      0.07   \n",
       "60997  0.73        4  ...        6        0      0.03      0.02      0.02   \n",
       "60998  0.70        7  ...       27        0      0.06      0.02      0.01   \n",
       "60999  0.61        7  ...       10        5      0.06      0.02      0.02   \n",
       "\n",
       "         spokPM     ficPM     magPM    newsPM    acadPM  \n",
       "0      45672.67  53310.74  53941.86  54042.73  62116.23  \n",
       "1      55701.50  37289.40  33259.44  33698.16  30795.96  \n",
       "2      26275.16  25860.15  26177.63  24585.28  30283.60  \n",
       "3      23374.72  25107.33  27541.93  27104.62  21727.08  \n",
       "4      20157.43  19270.82  25796.39  23557.38  37569.65  \n",
       "...         ...       ...       ...       ...       ...  \n",
       "60995      0.00      0.12      0.10      0.07      0.00  \n",
       "60996      0.00      0.02      0.01      0.02      0.03  \n",
       "60997      0.02      0.15      0.05      0.05      0.00  \n",
       "60998      0.00      0.02      0.02      0.22      0.00  \n",
       "60999      0.02      0.00      0.10      0.08      0.04  \n",
       "\n",
       "[61000 rows x 25 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>lemma</th>\n",
       "      <th>PoS</th>\n",
       "      <th>freq</th>\n",
       "      <th>perMil</th>\n",
       "      <th>%caps</th>\n",
       "      <th>%allC</th>\n",
       "      <th>range</th>\n",
       "      <th>disp</th>\n",
       "      <th>blog</th>\n",
       "      <th>...</th>\n",
       "      <th>news</th>\n",
       "      <th>acad</th>\n",
       "      <th>blogPM</th>\n",
       "      <th>webPM</th>\n",
       "      <th>TVMPM</th>\n",
       "      <th>spokPM</th>\n",
       "      <th>ficPM</th>\n",
       "      <th>magPM</th>\n",
       "      <th>newsPM</th>\n",
       "      <th>acadPM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9858</th>\n",
       "      <td>9859</td>\n",
       "      <td>folly</td>\n",
       "      <td>n</td>\n",
       "      <td>3795</td>\n",
       "      <td>3.82</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2893</td>\n",
       "      <td>0.95</td>\n",
       "      <td>543</td>\n",
       "      <td>...</td>\n",
       "      <td>504</td>\n",
       "      <td>518</td>\n",
       "      <td>4.37</td>\n",
       "      <td>5.94</td>\n",
       "      <td>1.62</td>\n",
       "      <td>1.6</td>\n",
       "      <td>4.18</td>\n",
       "      <td>4.45</td>\n",
       "      <td>4.14</td>\n",
       "      <td>4.32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      rank  lemma PoS  freq  perMil  %caps  %allC  range  disp  blog  ...  \\\n",
       "9858  9859  folly   n  3795    3.82   0.21   0.01   2893  0.95   543  ...   \n",
       "\n",
       "      news  acad  blogPM  webPM  TVMPM  spokPM  ficPM  magPM  newsPM  acadPM  \n",
       "9858   504   518    4.37   5.94   1.62     1.6   4.18   4.45    4.14    4.32  \n",
       "\n",
       "[1 rows x 25 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_data[freq_data['lemma'] == 'folly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>lemma</th>\n",
       "      <th>PoS</th>\n",
       "      <th>freq</th>\n",
       "      <th>perMil</th>\n",
       "      <th>%caps</th>\n",
       "      <th>%allC</th>\n",
       "      <th>range</th>\n",
       "      <th>disp</th>\n",
       "      <th>blog</th>\n",
       "      <th>...</th>\n",
       "      <th>news</th>\n",
       "      <th>acad</th>\n",
       "      <th>blogPM</th>\n",
       "      <th>webPM</th>\n",
       "      <th>TVMPM</th>\n",
       "      <th>spokPM</th>\n",
       "      <th>ficPM</th>\n",
       "      <th>magPM</th>\n",
       "      <th>newsPM</th>\n",
       "      <th>acadPM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14914</th>\n",
       "      <td>14915</td>\n",
       "      <td>foolishness</td>\n",
       "      <td>n</td>\n",
       "      <td>1781</td>\n",
       "      <td>1.79</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1534</td>\n",
       "      <td>0.93</td>\n",
       "      <td>311</td>\n",
       "      <td>...</td>\n",
       "      <td>105</td>\n",
       "      <td>98</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.71</td>\n",
       "      <td>1.39</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4.57</td>\n",
       "      <td>1.09</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        rank        lemma PoS  freq  perMil  %caps  %allC  range  disp  blog  \\\n",
       "14914  14915  foolishness   n  1781    1.79   0.03   0.01   1534  0.93   311   \n",
       "\n",
       "       ...  news  acad  blogPM  webPM  TVMPM  spokPM  ficPM  magPM  newsPM  \\\n",
       "14914  ...   105    98     2.5   2.71   1.39     0.5   4.57   1.09    0.86   \n",
       "\n",
       "       acadPM  \n",
       "14914    0.82  \n",
       "\n",
       "[1 rows x 25 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_data[freq_data['lemma'] == 'foolishness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>lemma</th>\n",
       "      <th>PoS</th>\n",
       "      <th>freq</th>\n",
       "      <th>perMil</th>\n",
       "      <th>%caps</th>\n",
       "      <th>%allC</th>\n",
       "      <th>range</th>\n",
       "      <th>disp</th>\n",
       "      <th>blog</th>\n",
       "      <th>...</th>\n",
       "      <th>news</th>\n",
       "      <th>acad</th>\n",
       "      <th>blogPM</th>\n",
       "      <th>webPM</th>\n",
       "      <th>TVMPM</th>\n",
       "      <th>spokPM</th>\n",
       "      <th>ficPM</th>\n",
       "      <th>magPM</th>\n",
       "      <th>newsPM</th>\n",
       "      <th>acadPM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>191</td>\n",
       "      <td>always</td>\n",
       "      <td>r</td>\n",
       "      <td>492943</td>\n",
       "      <td>496.41</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>182046</td>\n",
       "      <td>0.99</td>\n",
       "      <td>74076</td>\n",
       "      <td>...</td>\n",
       "      <td>44261</td>\n",
       "      <td>27114</td>\n",
       "      <td>596.17</td>\n",
       "      <td>504.87</td>\n",
       "      <td>646.53</td>\n",
       "      <td>505.19</td>\n",
       "      <td>683.52</td>\n",
       "      <td>437.45</td>\n",
       "      <td>363.56</td>\n",
       "      <td>226.35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     rank   lemma PoS    freq  perMil  %caps  %allC   range  disp   blog  ...  \\\n",
       "190   191  always   r  492943  496.41   0.04   0.01  182046  0.99  74076  ...   \n",
       "\n",
       "      news   acad  blogPM   webPM   TVMPM  spokPM   ficPM   magPM  newsPM  \\\n",
       "190  44261  27114  596.17  504.87  646.53  505.19  683.52  437.45  363.56   \n",
       "\n",
       "     acadPM  \n",
       "190  226.35  \n",
       "\n",
       "[1 rows x 25 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_data[(freq_data['lemma'] == 'always') & (freq_data['PoS'] == 'r')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = freq_data['perMil'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(data, bins=50)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_lexical_comp(text_tok_pos_lemma):\n",
    "    score = 0\n",
    "    for i in range(len(text_tok_pos_lemma)):\n",
    "        pos = text_tok_pos_lemma[i][1]\n",
    "        if pos == 'ADJ':\n",
    "            lemma = text_tok_pos_lemma[i][2]\n",
    "            score += math.log(freq_data[(freq_data['lemma'] == lemma) & (freq_data['PoS'] == 'j')]['perMil'])\n",
    "        if pos == 'ADV':\n",
    "            lemma = text_tok_pos_lemma[i][2]\n",
    "            score += math.log(freq_data[(freq_data['lemma'] == lemma) & (freq_data['PoS'] == 'r')]['perMil'])\n",
    "        if pos == 'NOUN':\n",
    "            lemma = text_tok_pos_lemma[i][2]\n",
    "            score += math.log(freq_data[(freq_data['lemma'] == lemma) & (freq_data['PoS'] == 'n')]['perMil'])\n",
    "        if pos == 'VERB':\n",
    "            lemma = text_tok_pos_lemma[i][2]\n",
    "            score += math.log(freq_data[(freq_data['lemma'] == lemma) & (freq_data['PoS'] == 'v')]['perMil'])\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.911175975879146\n",
      "8.342509416366052\n"
     ]
    }
   ],
   "source": [
    "added_words = [['ADD_stop', 'VERB', 'stop']]\n",
    "deleted_words = [['make', 'VERB', 'make'], ['something', 'PRON', 'something'], ['to', 'PART', 'to'], ['defuse', 'VERB', 'defuse']]\n",
    "print(calc_lexical_comp(added_words))\n",
    "print(calc_lexical_comp(deleted_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_del_score(words):\n",
    "    return len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_influence_scores_to_edit_sequence(edits, ad_spans, d_spans, a_spans, edit_sequence, text_to_edit, sent1_tok):\n",
    "    influence_scores = []\n",
    "    for edit_span_id in edit_sequence:\n",
    "\n",
    "        if edit_span_id[0] == 'splr_span' or edit_span_id[0] == 'mvr_span':\n",
    "            edited_text_tok = []\n",
    "            ad_span_idx = edit_span_id[1]\n",
    "            ad_span = list(range(ad_spans[ad_span_idx][0], ad_spans[ad_span_idx][1]+1))\n",
    "            for i in range(len(text_to_edit)):\n",
    "                if i in ad_span:\n",
    "                    if edits[i] == 'DEL':\n",
    "                        continue\n",
    "                    else:\n",
    "                        edited_text_tok.append(text_to_edit[i][0][4:])\n",
    "                else:\n",
    "                    if edits[i] == 'DEL' or edits[i] == 'KEEP':\n",
    "                        edited_text_tok.append(text_to_edit[i][0])\n",
    "                    else:\n",
    "                        continue\n",
    "            syn_score = calc_max_deptree_depth(\" \".join(sent1_tok)) - calc_max_deptree_depth(\" \".join(edited_text_tok))\n",
    "            if edit_span_id[0] == 'splr_span':\n",
    "                influence_scores.append(['splr_span', edit_span_id[1], syn_score])\n",
    "            elif edit_span_id[0] == 'mvr_span':\n",
    "                influence_scores.append(['mvr_span', edit_span_id[1], syn_score])\n",
    "        \n",
    "        if edit_span_id[0] == 'rep_span':\n",
    "            added_words = []\n",
    "            deleted_words = []\n",
    "            ad_span_idx = edit_span_id[1]\n",
    "            ad_span = list(range(ad_spans[ad_span_idx][0], ad_spans[ad_span_idx][1]+1))\n",
    "            for i in range(len(text_to_edit)):\n",
    "                if i in ad_span:\n",
    "                    if edits[i] == 'DEL':\n",
    "                        deleted_words.append(text_to_edit[i])\n",
    "                    else:\n",
    "                        added_words.append(text_to_edit[i])            \n",
    "            rep_score = calc_lexical_comp(deleted_words) - calc_lexical_comp(added_words)\n",
    "            influence_scores.append(['rep_span', edit_span_id[1], rep_score])\n",
    "        \n",
    "        if edit_span_id[0] == 'd_span':\n",
    "            deleted_words = []\n",
    "            d_span_idx = edit_span_id[1]\n",
    "            d_span = list(range(d_spans[d_span_idx][0], d_spans[d_span_idx][1]+1))\n",
    "            for i in range(len(text_to_edit)):\n",
    "                if i in d_span:\n",
    "                    deleted_words.append(text_to_edit[i])         \n",
    "            del_score = calc_del_score(deleted_words)\n",
    "            influence_scores.append(['d_span', edit_span_id[1], del_score])\n",
    "        \n",
    "        if edit_span_id[0] == 'a_span':\n",
    "            influence_scores.append(['a_span', edit_span_id[1], 0])\n",
    "\n",
    "    return influence_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_edit_to_text(edits, ad_spans, d_spans, a_spans, splr_ids, rep_ids, mvr_ids, d_ids, a_ids, text_to_edit, sent1_tok):\n",
    "    edit_sequence = splr_ids + rep_ids + mvr_ids + d_ids + a_ids\n",
    "    edit_sequence = append_influence_scores_to_edit_sequence(edits, ad_spans, d_spans, a_spans, edit_sequence, text_to_edit, sent1_tok)\n",
    "    edited_texts = []\n",
    "    for bit in range(1<<len(edit_sequence)):\n",
    "        apply_edit_sequence = []\n",
    "        for i in range(len(edit_sequence)):\n",
    "            mask = 1 << i\n",
    "            if bit&mask:\n",
    "                apply_edit_sequence += [edit_sequence[i]]\n",
    "        #print(apply_edit_sequence)\n",
    "\n",
    "        edited_text = []    \n",
    "        seen_text_to_edit = [0 for i in range(len(text_to_edit))]\n",
    "        influence_score = 0\n",
    "        for apply_edit in apply_edit_sequence:\n",
    "            if apply_edit[0] == 'splr_span':\n",
    "                influence_score += apply_edit[2]\n",
    "                edited_text_for_calc_score = []\n",
    "                ad_span_idx = apply_edit[1]\n",
    "                ad_span = list(range(ad_spans[ad_span_idx][0], ad_spans[ad_span_idx][1]+1))\n",
    "                for i in range(len(text_to_edit)):\n",
    "                    if i in ad_span:\n",
    "                        if edits[i] == 'DEL':\n",
    "                            continue\n",
    "                        else:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "splr_ids = extract_splr_ids(edits, ad_spans, sent2_tok)\n",
    "rep_ids = extract_rep_ids(edits, ad_spans, sent2_tok, aligns, splr_ids)\n",
    "\n",
    "mvr_ids = extract_mvr_ids(edits, ad_spans, d_spans, a_spans, sent2_tok, aligns)\n",
    "d_ids = extract_d_ids(edits, d_spans, sent2_tok, aligns)\n",
    "a_ids = extract_a_ids(a_spans, mvr_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['splr_span', 1], ['splr_span', 3], ['rep_span', 0], ['rep_span', 2], ['mvr_span', ('d_span', 0)], ['a_span', 0], ['a_span', 1]]\n",
      "[['splr_span', 1]]\n",
      "[['splr_span', 3]]\n",
      "[['splr_span', 1], ['splr_span', 3]]\n",
      "[['rep_span', 0]]\n",
      "[['splr_span', 1], ['rep_span', 0]]\n",
      "[['splr_span', 3], ['rep_span', 0]]\n",
      "[['splr_span', 1], ['splr_span', 3], ['rep_span', 0]]\n",
      "[['rep_span', 2]]\n",
      "[['splr_span', 1], ['rep_span', 2]]\n",
      "[['splr_span', 3], ['rep_span', 2]]\n",
      "[['splr_span', 1], ['splr_span', 3], ['rep_span', 2]]\n",
      "[['rep_span', 0], ['rep_span', 2]]\n",
      "[['splr_span', 1], ['rep_span', 0], ['rep_span', 2]]\n",
      "[['splr_span', 3], ['rep_span', 0], ['rep_span', 2]]\n",
      "[['splr_span', 1], ['splr_span', 3], ['rep_span', 0], ['rep_span', 2]]\n",
      "[['mvr_span', ('d_span', 0)]]\n",
      "[['splr_span', 1], ['mvr_span', ('d_span', 0)]]\n",
      "[['splr_span', 3], ['mvr_span', ('d_span', 0)]]\n",
      "[['splr_span', 1], ['splr_span', 3], ['mvr_span', ('d_span', 0)]]\n",
      "[['rep_span', 0], ['mvr_span', ('d_span', 0)]]\n",
      "[['splr_span', 1], ['rep_span', 0], ['mvr_span', ('d_span', 0)]]\n",
      "[['splr_span', 3], ['rep_span', 0], ['mvr_span', ('d_span', 0)]]\n",
      "[['splr_span', 1], ['splr_span', 3], ['rep_span', 0], ['mvr_span', ('d_span', 0)]]\n",
      "[['rep_span', 2], ['mvr_span', ('d_span', 0)]]\n",
      "[['splr_span', 1], ['rep_span', 2], ['mvr_span', ('d_span', 0)]]\n",
      "[['splr_span', 3], ['rep_span', 2], ['mvr_span', ('d_span', 0)]]\n",
      "[['splr_span', 1], ['splr_span', 3], ['rep_span', 2], ['mvr_span', ('d_span', 0)]]\n",
      "[['rep_span', 0], ['rep_span', 2], ['mvr_span', ('d_span', 0)]]\n",
      "[['splr_span', 1], ['rep_span', 0], ['rep_span', 2], ['mvr_span', ('d_span', 0)]]\n",
      "[['splr_span', 3], ['rep_span', 0], ['rep_span', 2], ['mvr_span', ('d_span', 0)]]\n",
      "[['splr_span', 1], ['splr_span', 3], ['rep_span', 0], ['rep_span', 2], ['mvr_span', ('d_span', 0)]]\n",
      "[['a_span', 0]]\n",
      "[['splr_span', 1], ['a_span', 0]]\n",
      "[['splr_span', 3], ['a_span', 0]]\n",
      "[['splr_span', 1], ['splr_span', 3], ['a_span', 0]]\n",
      "[['rep_span', 0], ['a_span', 0]]\n",
      "[['splr_span', 1], ['rep_span', 0], ['a_span', 0]]\n",
      "[['splr_span', 3], ['rep_span', 0], ['a_span', 0]]\n",
      "[['splr_span', 1], ['splr_span', 3], ['rep_span', 0], ['a_span', 0]]\n",
      "[['rep_span', 2], ['a_span', 0]]\n",
      "[['splr_span', 1], ['rep_span', 2], ['a_span', 0]]\n",
      "[['splr_span', 3], ['rep_span', 2], ['a_span', 0]]\n",
      "[['splr_span', 1], ['splr_span', 3], ['rep_span', 2], ['a_span', 0]]\n",
      "[['rep_span', 0], ['rep_span', 2], ['a_span', 0]]\n",
      "[['splr_span', 1], ['rep_span', 0], ['rep_span', 2], ['a_span', 0]]\n",
      "[['splr_span', 3], ['rep_span', 0], ['rep_span', 2], ['a_span', 0]]\n",
      "[['splr_span', 1], ['splr_span', 3], ['rep_span', 0], ['rep_span', 2], ['a_span', 0]]\n",
      "[['mvr_span', ('d_span', 0)], ['a_span', 0]]\n",
      "[['splr_span', 1], ['mvr_span', ('d_span', 0)], ['a_span', 0]]\n",
      "[['splr_span', 3], ['mvr_span', ('d_span', 0)], ['a_span', 0]]\n",
      "[['splr_span', 1], ['splr_span', 3], ['mvr_span', ('d_span', 0)], ['a_span', 0]]\n",
      "[['rep_span', 0], ['mvr_span', ('d_span', 0)], ['a_span', 0]]\n",
      "[['splr_span', 1], ['rep_span', 0], ['mvr_span', ('d_span', 0)], ['a_span', 0]]\n",
      "[['splr_span', 3], ['rep_span', 0], ['mvr_span', ('d_span', 0)], ['a_span', 0]]\n",
      "[['splr_span', 1], ['splr_span', 3], ['rep_span', 0], ['mvr_span', ('d_span', 0)], ['a_span', 0]]\n",
      "[['rep_span', 2], ['mvr_span', ('d_span', 0)], ['a_span', 0]]\n",
      "[['splr_span', 1], ['rep_span', 2], ['mvr_span', ('d_span', 0)], ['a_span', 0]]\n",
      "[['splr_span', 3], ['rep_span', 2], ['mvr_span', ('d_span', 0)], ['a_span', 0]]\n",
      "[['splr_span', 1], ['splr_span', 3], ['rep_span', 2], ['mvr_span', ('d_span', 0)], ['a_span', 0]]\n",
      "[['rep_span', 0], ['rep_span', 2], ['mvr_span', ('d_span', 0)], ['a_span', 0]]\n",
      "[['splr_span', 1], ['rep_span', 0], ['rep_span', 2], ['mvr_span', ('d_span', 0)], ['a_span', 0]]\n",
      "[['splr_span', 3], ['rep_span', 0], ['rep_span', 2], ['mvr_span', ('d_span', 0)], ['a_span', 0]]\n",
      "[['splr_span', 1], ['splr_span', 3], ['rep_span', 0], ['rep_span', 2], ['mvr_span', ('d_span', 0)], ['a_span', 0]]\n",
      "[['a_span', 1]]\n",
      "[['splr_span', 1], ['a_span', 1]]\n",
      "[['splr_span', 3], ['a_span', 1]]\n",
      "[['splr_span', 1], ['splr_span', 3], ['a_span', 1]]\n",
      "[['rep_span', 0], ['a_span', 1]]\n",
      "[['splr_span', 1], ['rep_span', 0], ['a_span', 1]]\n",
      "[['splr_span', 3], ['rep_span', 0], ['a_span', 1]]\n",
      "[['splr_span', 1], ['splr_span', 3], ['rep_span', 0], ['a_span', 1]]\n",
      "[['rep_span', 2], ['a_span', 1]]\n",
      "[['splr_span', 1], ['rep_span', 2], ['a_span', 1]]\n",
      "[['splr_span', 3], ['rep_span', 2], ['a_span', 1]]\n",
      "[['splr_span', 1], ['splr_span', 3], ['rep_span', 2], ['a_span', 1]]\n",
      "[['rep_span', 0], ['rep_span', 2], ['a_span', 1]]\n",
      "[['splr_span', 1], ['rep_span', 0], ['rep_span', 2], ['a_span', 1]]\n",
      "[['splr_span', 3], ['rep_span', 0], ['rep_span', 2], ['a_span', 1]]\n",
      "[['splr_span', 1], ['splr_span', 3], ['rep_span', 0], ['rep_span', 2], ['a_span', 1]]\n",
      "[['mvr_span', ('d_span', 0)], ['a_span', 1]]\n",
      "[['splr_span', 1], ['mvr_span', ('d_span', 0)], ['a_span', 1]]\n",
      "[['splr_span', 3], ['mvr_span', ('d_span', 0)], ['a_span', 1]]\n",
      "[['splr_span', 1], ['splr_span', 3], ['mvr_span', ('d_span', 0)], ['a_span', 1]]\n",
      "[['rep_span', 0], ['mvr_span', ('d_span', 0)], ['a_span', 1]]\n",
      "[['splr_span', 1], ['rep_span', 0], ['mvr_span', ('d_span', 0)], ['a_span', 1]]\n",
      "[['splr_span', 3], ['rep_span', 0], ['mvr_span', ('d_span', 0)], ['a_span', 1]]\n",
      "[['splr_span', 1], ['splr_span', 3], ['rep_span', 0], ['mvr_span', ('d_span', 0)], ['a_span', 1]]\n",
      "[['rep_span', 2], ['mvr_span', ('d_span', 0)], ['a_span', 1]]\n",
      "[['splr_span', 1], ['rep_span', 2], ['mvr_span', ('d_span', 0)], ['a_span', 1]]\n",
      "[['splr_span', 3], ['rep_span', 2], ['mvr_span', ('d_span', 0)], ['a_span', 1]]\n",
      "[['splr_span', 1], ['splr_span', 3], ['rep_span', 2], ['mvr_span', ('d_span', 0)], ['a_span', 1]]\n",
      "[['rep_span', 0], ['rep_span', 2], ['mvr_span', ('d_span', 0)], ['a_span', 1]]\n",
      "[['splr_span', 1], ['rep_span', 0], ['rep_span', 2], ['mvr_span', ('d_span', 0)], ['a_span', 1]]\n",
      "[['splr_span', 3], ['rep_span', 0], ['rep_span', 2], ['mvr_span', ('d_span', 0)], ['a_span', 1]]\n",
      "[['splr_span', 1], ['splr_span', 3], ['rep_span', 0], ['rep_span', 2], ['mvr_span', ('d_span', 0)], ['a_span', 1]]\n",
      "[['a_span', 0], ['a_span', 1]]\n",
      "[['splr_span', 1], ['a_span', 0], ['a_span', 1]]\n",
      "[['splr_span', 3], ['a_span', 0], ['a_span', 1]]\n",
      "[['splr_span', 1], ['splr_span', 3], ['a_span', 0], ['a_span', 1]]\n",
      "[['rep_span', 0], ['a_span', 0], ['a_span', 1]]\n",
      "[['splr_span', 1], ['rep_span', 0], ['a_span', 0], ['a_span', 1]]\n",
      "[['splr_span', 3], ['rep_span', 0], ['a_span', 0], ['a_span', 1]]\n",
      "[['splr_span', 1], ['splr_span', 3], ['rep_span', 0], ['a_span', 0], ['a_span', 1]]\n",
      "[['rep_span', 2], ['a_span', 0], ['a_span', 1]]\n",
      "[['splr_span', 1], ['rep_span', 2], ['a_span', 0], ['a_span', 1]]\n",
      "[['splr_span', 3], ['rep_span', 2], ['a_span', 0], ['a_span', 1]]\n",
      "[['splr_span', 1], ['splr_span', 3], ['rep_span', 2], ['a_span', 0], ['a_span', 1]]\n",
      "[['rep_span', 0], ['rep_span', 2], ['a_span', 0], ['a_span', 1]]\n",
      "[['splr_span', 1], ['rep_span', 0], ['rep_span', 2], ['a_span', 0], ['a_span', 1]]\n",
      "[['splr_span', 3], ['rep_span', 0], ['rep_span', 2], ['a_span', 0], ['a_span', 1]]\n",
      "[['splr_span', 1], ['splr_span', 3], ['rep_span', 0], ['rep_span', 2], ['a_span', 0], ['a_span', 1]]\n",
      "[['mvr_span', ('d_span', 0)], ['a_span', 0], ['a_span', 1]]\n",
      "[['splr_span', 1], ['mvr_span', ('d_span', 0)], ['a_span', 0], ['a_span', 1]]\n",
      "[['splr_span', 3], ['mvr_span', ('d_span', 0)], ['a_span', 0], ['a_span', 1]]\n",
      "[['splr_span', 1], ['splr_span', 3], ['mvr_span', ('d_span', 0)], ['a_span', 0], ['a_span', 1]]\n",
      "[['rep_span', 0], ['mvr_span', ('d_span', 0)], ['a_span', 0], ['a_span', 1]]\n",
      "[['splr_span', 1], ['rep_span', 0], ['mvr_span', ('d_span', 0)], ['a_span', 0], ['a_span', 1]]\n",
      "[['splr_span', 3], ['rep_span', 0], ['mvr_span', ('d_span', 0)], ['a_span', 0], ['a_span', 1]]\n",
      "[['splr_span', 1], ['splr_span', 3], ['rep_span', 0], ['mvr_span', ('d_span', 0)], ['a_span', 0], ['a_span', 1]]\n",
      "[['rep_span', 2], ['mvr_span', ('d_span', 0)], ['a_span', 0], ['a_span', 1]]\n",
      "[['splr_span', 1], ['rep_span', 2], ['mvr_span', ('d_span', 0)], ['a_span', 0], ['a_span', 1]]\n",
      "[['splr_span', 3], ['rep_span', 2], ['mvr_span', ('d_span', 0)], ['a_span', 0], ['a_span', 1]]\n",
      "[['splr_span', 1], ['splr_span', 3], ['rep_span', 2], ['mvr_span', ('d_span', 0)], ['a_span', 0], ['a_span', 1]]\n",
      "[['rep_span', 0], ['rep_span', 2], ['mvr_span', ('d_span', 0)], ['a_span', 0], ['a_span', 1]]\n",
      "[['splr_span', 1], ['rep_span', 0], ['rep_span', 2], ['mvr_span', ('d_span', 0)], ['a_span', 0], ['a_span', 1]]\n",
      "[['splr_span', 3], ['rep_span', 0], ['rep_span', 2], ['mvr_span', ('d_span', 0)], ['a_span', 0], ['a_span', 1]]\n"
     ]
    }
   ],
   "source": [
    "edit_sequence = splr_ids + rep_ids + mvr_ids + d_ids + a_ids\n",
    "print(edit_sequence)\n",
    "for bit in range(1<<len(edit_sequence)):\n",
    "    if bit == 0 or bit == (1<<len(edit_sequence))-1:\n",
    "        continue\n",
    "    else:\n",
    "        apply_edit_sequence = []\n",
    "        for i in range(len(edit_sequence)):\n",
    "            mask = 1 << i\n",
    "            if bit&mask:\n",
    "                apply_edit_sequence += [edit_sequence[i]]\n",
    "        print(apply_edit_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['splr_span', 1], ['splr_span', 3]]\n",
      "[['rep_span', 0], ['rep_span', 2]]\n",
      "[['mvr_span', ('d_span', 0)]]\n",
      "[]\n",
      "[['a_span', 0], ['a_span', 1]]\n"
     ]
    }
   ],
   "source": [
    "print(splr_ids)\n",
    "print(rep_ids)\n",
    "print(mvr_ids)\n",
    "print(d_ids)\n",
    "print(a_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_rep_span(edits, ad_spans, aligns, sent1_tok, sent2_tok, slp_spans):\n",
    "    d_starts = extract_d_starts(edits, ad_spans)\n",
    "    aligns_sent1_ids = [i[0] for i in aligns]\n",
    "    aligns_sent2_ids = [i[1] for i in aligns]\n",
    "    for span, d_start in zip(ad_spans, d_starts):\n",
    "        d_seen = [0 for i in range(len(d_start))]\n",
    "        a_wordlist = [sent2_tok[i] for i in range(span[0], d_start)]\n",
    "        edit_to_inserts = []\n",
    "        #d_pointer = d_start\n",
    "        for d_pointer in range(d_start, span[1]):\n",
    "            for i in range(len(aligns_sent1_ids)):\n",
    "                if d_pointer in aligns_sent1_ids[i] and d_seen[d_pointer - d_start] != 0:\n",
    "                    d_seen[d_pointer - d_start] = 1\n",
    "                    sent2_ids = aligns_sent2_ids[i]\n",
    "                    sent2_words = [sent2_tok[i] for i in sent2_ids]\n",
    "                    edit_to_insert = ['REP-S']\n",
    "                    for sent2_word in sent2_words:\n",
    "                        if sent2_word in a_wordlist:\n",
    "                            edit_to_insert.append(sent2_word)\n",
    "                    edit_to_insert.append('REP-E')\n",
    "                    if len(edit_to_insert) > 2:\n",
    "                        edit_to_inserts.append(edit_to_insert)\n",
    "\n",
    "\n",
    "        for i in range(len(aligns_sent1_ids)):\n",
    "            if d_pointer in aligns_sent1_ids[i] and d_seen[d_pointer - d_start] != 0:\n",
    "                d_seen[d_pointer - d_start] = 1\n",
    "                sent2_ids = aligns_sent2_ids[i]\n",
    "                sent2_words = [sent2_tok[i] for i in sent2_ids]\n",
    "                edit_to_insert = ['REP-S']\n",
    "                for sent2_word in sent2_words:\n",
    "                    if sent2_word in a_wordlist:\n",
    "                        edit_to_insert.append(sent2_word)\n",
    "                edit_to_insert.append('REP-E')\n",
    "                if len(edit_to_insert) > 2:\n",
    "                    edit_to_inserts.append(edit_to_insert)\n",
    "            d_pointer += 1\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8a7e92afde7a2b8cf7f9e7c82533dd48faf2b1d8196f2f255757b260712621ce"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('3.8.5': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
