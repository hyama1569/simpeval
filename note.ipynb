{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from transformers import BertTokenizer\n",
    "from collections import namedtuple\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from neural_jacana.model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_texts(texts):\n",
    "    tokenized_texts = []\n",
    "    for text in texts:\n",
    "        tokenized_texts.append(nltk.word_tokenize(re.sub(r'[\\(\\)\\`\\'\\\"]', '', text)))\n",
    "    return tokenized_texts\n",
    "\n",
    "def get_unique_list(seq):\n",
    "    seen = []\n",
    "    return [x for x in seq if x not in seen and not seen.append(x)]\n",
    "\n",
    "def check_inclusion(list1, list2):\n",
    "    flag = False\n",
    "    for i in list1:\n",
    "        if i in list2:\n",
    "            flag = True\n",
    "    for i in list2:\n",
    "        if i in list1:\n",
    "            flag = True\n",
    "    return flag\n",
    "\n",
    "def merge_sent2_ids(align_id_pairs):\n",
    "    '''\n",
    "    merge sent2 ids aligned to sent1 ids.\n",
    "    '''\n",
    "    merged_sent2_align_id_pairs = []\n",
    "    sorted_align_id_pairs = sorted(align_id_pairs, key=lambda x:(int(re.findall(r\"\\d+\", x)[0]), int(re.findall(r\"\\d+\", x)[1])))\n",
    "    tuple_pairs = [(int(re.findall(r\"\\d+\", i)[0]), int(re.findall(r\"\\d+\", i)[1])) for i in sorted_align_id_pairs]\n",
    "    for pair in tuple_pairs:\n",
    "        keys = [i[0] for i in merged_sent2_align_id_pairs]\n",
    "        keys_flatten = [x for row in keys for x in row]\n",
    "        if pair[0] not in keys_flatten:\n",
    "            merged_sent2_align_id_pairs.append(([pair[0]], [pair[1]]))\n",
    "        else:\n",
    "            ind_addval = [i for i in range(len(keys)) if pair[0] in keys[i]][0]\n",
    "            merged_sent2_align_id_pairs[ind_addval][1].append(pair[1])\n",
    "    return merged_sent2_align_id_pairs\n",
    "\n",
    "def merge_sent1_ids(merged_sent2_align_id_pairs):\n",
    "    '''\n",
    "    merge sent1 ids having the same sent2 ids.\n",
    "    '''\n",
    "    merged_sent1_align_id_pairs = []\n",
    "    dup_inds = []\n",
    "    vals = [pair[1] for pair in merged_sent2_align_id_pairs]\n",
    "    for pair in merged_sent2_align_id_pairs:\n",
    "        dup_ind = [i for i, x in enumerate(vals) if x == pair[1]]\n",
    "        if len(dup_ind) > 1:\n",
    "            dup_inds.append(dup_ind)\n",
    "    dup_inds = get_unique_list(dup_inds)\n",
    "\n",
    "    if len(dup_inds) != 0: #if there are duplicate values in merged_sent2_align_id_pairs, they should be merged.\n",
    "        keys_to_add = []\n",
    "        for i in range(len(dup_inds)):\n",
    "            key_to_add = []\n",
    "            for j in range(len(merged_sent2_align_id_pairs)):\n",
    "                if j in dup_inds[i]:\n",
    "                    key_to_add.append(merged_sent2_align_id_pairs[j][0][0])\n",
    "            if len(key_to_add) != 0:\n",
    "                keys_to_add.append(key_to_add)\n",
    "        \n",
    "        pairs_to_add = []\n",
    "        for i in range(len(dup_inds)):\n",
    "            pairs_to_add.append((keys_to_add[i], merged_sent2_align_id_pairs[dup_inds[i][0]][1]))\n",
    "\n",
    "        dup_inds_flatten = [x for row in dup_inds for x in row]\n",
    "        for i in range(len(merged_sent2_align_id_pairs)):\n",
    "            if i not in dup_inds_flatten:\n",
    "                merged_sent1_align_id_pairs.append(merged_sent2_align_id_pairs[i])\n",
    "        merged_sent1_align_id_pairs.extend(pairs_to_add)\n",
    "        return merged_sent1_align_id_pairs\n",
    "    \n",
    "    else:\n",
    "        return merged_sent2_align_id_pairs\n",
    "\n",
    "def merge_align_ids_crossing(merged_ids):\n",
    "    sent1_ids = [pair[0] for pair in merged_ids]\n",
    "    sent2_ids = [pair[1] for pair in merged_ids]\n",
    "    res = []\n",
    "    added_sent1 = [0 for i in range(len(sent1_ids))]\n",
    "    for i in range(len(sent1_ids)):\n",
    "        sent2_ids_to_add = sent2_ids[i]\n",
    "        sent1_correspond = sent1_ids[i]\n",
    "        for j in range(i, len(sent1_ids)):\n",
    "            if check_inclusion(sent1_ids[i], sent1_ids[j]) == True:\n",
    "                if added_sent1[j] == 0 and i != j:\n",
    "                    added_sent1[j] = 1\n",
    "                    sent2_ids_to_add.extend(sent2_ids[j])\n",
    "                    sent1_correspond.extend(sent1_ids[j])\n",
    "                    sent1_correspond = get_unique_list(sent1_correspond)\n",
    "        if len(sent2_ids_to_add) > 1 and added_sent1 == 0:\n",
    "            res.append((sent1_correspond, sent2_ids_to_add))\n",
    "            added_sent1[i] = 1\n",
    "            #print(0)\n",
    "\n",
    "    added_sent2 = [0 for i in range(len(sent2_ids))]\n",
    "    for i in range(len(sent2_ids)):\n",
    "        sent1_ids_to_add = sent1_ids[i]\n",
    "        sent2_correspond = sent2_ids[i]\n",
    "        for j in range(i, len(sent2_ids)):\n",
    "            if check_inclusion(sent2_ids[i], sent2_ids[j]) == True:\n",
    "                if added_sent2[j] == 0 and i != j:\n",
    "                    added_sent2[j] = 1\n",
    "                    sent1_ids_to_add.extend(sent1_ids[j])\n",
    "                    sent2_correspond.extend(sent2_ids[j])\n",
    "                    sent2_correspond = get_unique_list(sent2_correspond)\n",
    "        if len(sent1_ids_to_add) > 1 and added_sent2[i] == 0:\n",
    "            res.append((sent1_ids_to_add, sent2_correspond))\n",
    "            added_sent2[i] = 1\n",
    "            #print(1)\n",
    "\n",
    "    for i in range(len(merged_ids)):\n",
    "        if added_sent1[i] == 0 and added_sent2[i] == 0:\n",
    "            res.append((sent1_ids[i], sent2_ids[i]))\n",
    "            #print(2)\n",
    "    #print(added_sent1, added_sent2)\n",
    "    return res\n",
    "\n",
    "def ids_to_words(merged_id_pairs, tokenized_sent1, tokenized_sent2):\n",
    "    align_word_pairs = []\n",
    "    for pair in merged_id_pairs:\n",
    "        sent1_words = [tokenized_sent1[i] for i in pair[0]]\n",
    "        sent2_words = [tokenized_sent2[i] for i in pair[1]]\n",
    "        align_word_pairs.append((sent1_words, sent2_words))\n",
    "    return align_word_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ['0-0', '1-0', '2-0', '1-1', '2-2', '3-3']\n",
    "a = ['0-0', '1-0', '1-1', '2-0', '2-2', '3-3', '3-4', '3-5', '4-4', '5-5', '6-6', '7-7']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([0], [0]),\n",
       " ([1], [0, 1]),\n",
       " ([2], [0, 2]),\n",
       " ([3], [3, 4, 5]),\n",
       " ([4], [4]),\n",
       " ([5], [5]),\n",
       " ([6], [6]),\n",
       " ([7], [7])]"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_sent2_ids(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([0, 1, 2], [0, 1, 2]), ([3, 4, 5], [3, 4, 5]), ([6], [6]), ([7], [7])]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_align_ids_crossing(merge_sent1_ids(merge_sent2_ids(a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at neural_jacana/spanbert_hf_base and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/yamanaka.h.ac/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed test examples 0/1\n",
      "[([1, 2], [8]), ([3], [6, 7]), ([4], [0]), ([5], [1]), ([6], [2]), ([7], [3]), ([8], [4]), ([9], [9])] [(['Radar', 'observations'], ['radar']), (['indicate'], ['observed', 'by']), (['a'], ['A']), (['fairly'], ['mainly']), (['pure'], ['pure']), (['iron-nickel'], ['Iron-Nickel']), (['composition'], ['composition']), (['.'], ['.'])]\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--batchsize\", default=1, type=int)\n",
    "parser.add_argument(\"--learning_rate\", default=1e-5, type=float)\n",
    "parser.add_argument(\"--max_epoch\", default=6, type=int)\n",
    "parser.add_argument(\"--max_span_size\", default=4, type=int)\n",
    "parser.add_argument(\"--max_seq_length\", default=128, type=int)\n",
    "parser.add_argument(\"--max_sent_length\", default=70, type=int)\n",
    "parser.add_argument(\"--seed\", default=1234, type=int)\n",
    "parser.add_argument(\"--dataset\", default='mtref', type=str)\n",
    "parser.add_argument(\"--sure_and_possible\", default='True', type=str)\n",
    "parser.add_argument(\"--distance_embedding_size\", default=128, type=int)\n",
    "parser.add_argument(\"--use_transition_layer\", default='False', type=str, help='if False, will set transition score to 0.')\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = NeuralWordAligner(args)\n",
    "my_device = torch.device('cpu')\n",
    "model = model.to(my_device)\n",
    "\n",
    "checkpoint = torch.load('./neural_jacana/Checkpoint_sure_and_possible_True_dataset_mtref_batchsize_1_max_span_size_4_use_transition_layer_False_epoch_2_0.9150.pt', map_location=my_device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "#sources = [\"Military experts say the line between combat is getting blurry.\", \"Their eyes are quite small, and their visual acuity is poor.\", \n",
    "#            \"According to Ledford, Northrop executives said they would build substantial parts of the bomber in Palmdale, creating about 1,500 jobs.\",\n",
    "#            \"In return, Rollo swore fealty to Charles, converted to Christianity, and undertook to defend the northern region of France against the Incursions of other Viking groups.\",\n",
    "#            \"A fee is the price one pays as remuneration for services, especially the Honorarium paid to a doctor, lawyer, consultant, or other member of a learned profession.\",\n",
    "#            \"Thereafter the county's administration was conducted at Duns or Lauder until Greenlaw became the county town in 1596.\",\n",
    "#            \"MacGruber starts asking for simple objects to make something to defuse the bomb, but he is later distracted by something (usually involving his personal life) that makes him run out of time.\",\n",
    "#            \"As the largest sub-region in Mesoamerica, it encompassed a vast and varied landscape, from the mountainous regions of the Sierra Madre to the semi-arid plains of northern Yucatán.\",\n",
    "#            \"Together they formed New Music Manchester, a group committed to contemporary music.\"]\n",
    "#targets = [\"Military experts say war is changing.\", \"Their eyes are very small, and they do not see well.\",\n",
    "#            \"According to Ledford, Northrop said they would build most of the bomber parts in Palmdale. It would create 1,500 jobs.\",\n",
    "#            \"Rollo swore to be loyal to Charles, then he changed his religion to Christianity. Rollo protected northern France by fighting Viking invaders.\",\n",
    "#            \"A price one might pay for services is a called a fee.\",\n",
    "#            \"After that, the county offices were at Duns or Lauder. In 1596 they moved to Greenlaw.\",\n",
    "#            \"Macgruber starts by asking for simple objects to stop the bomb from working. later he is distracted by an event from his personal life. as a result, he runs out of time to stop the bomb.\",\n",
    "#            \"As the largest sub-region in Mesoamerica, it was a vast and varied landscape.\",\n",
    "#            \"Both of the formed a group committed to contemporary music called new music Manchester.\"]\n",
    "#sources = [\"As the largest sub-region in Mesoamerica, it encompassed a vast and varied landscape, from the mountainous regions of the Sierra Madre to the semi-arid plains of northern Yucatán.\"]\n",
    "#targets = [\"As the largest sub-region in Mesoamerica, it was a vast and varied landscape.\"]\n",
    "#sources = ['Together they formed New Music Manchester, a group committed to contemporary music.']\n",
    "#targets = ['Both of the formed a group committed to contemporary music called new music Manchester.']\n",
    "sources = ['Characteristics Radar observations indicate a fairly pure iron-nickel composition.']\n",
    "targets = ['A mainly pure Iron-Nickel composition was observed by radar.']\n",
    "#sources = [\"MacGruber starts asking for simple objects to make something to defuse the bomb, but he is later distracted by something (usually involving his personal life) that makes him run out of time.\"]\n",
    "#targets = [\"Macgruber starts by asking for simple objects to stop the bomb from working. later he is distracted by an event from his personal life. as a result, he runs out of time to stop the bomb.\"]\n",
    "nltk.download('punkt')\n",
    "tokenized_sources = preprocess_texts(sources)\n",
    "tokenized_targets = preprocess_texts(targets)\n",
    "\n",
    "data = []\n",
    "example = namedtuple('example', 'ID, text_a, text_b, label')\n",
    "for i, (tokenized_source, tokenized_target) in enumerate(zip(tokenized_sources, tokenized_targets)):\n",
    "    data.append(example(i, ' '.join(tokenized_source), ' '.join(tokenized_target), '0-0'))\n",
    "test_dataloader = create_Data_Loader(data_examples=data, args=args, set_type='test', batchsize=1, max_seq_length=128, tokenizer=tokenizer)\n",
    "\n",
    "for step, batch in enumerate(test_dataloader):\n",
    "    batch = tuple(t.to(my_device) for t in batch)\n",
    "    input_ids_a_and_b, input_ids_b_and_a, input_mask, segment_ids_a_and_b, segment_ids_b_and_a, sent1_valid_ids, sent2_valid_ids, sent1_wordpiece_length, sent2_wordpiece_length = batch\n",
    "    with torch.no_grad():\n",
    "        decoded_results = model(input_ids_a_and_b=input_ids_a_and_b, input_ids_b_and_a=input_ids_b_and_a,\n",
    "                                    attention_mask=input_mask, token_type_ids_a_and_b=segment_ids_a_and_b,\n",
    "                                    token_type_ids_b_and_a=segment_ids_b_and_a,\n",
    "                                    sent1_valid_ids=sent1_valid_ids, sent2_valid_ids=sent2_valid_ids,\n",
    "                                    sent1_wordpiece_length=sent1_wordpiece_length,\n",
    "                                    sent2_wordpiece_length=sent2_wordpiece_length)\n",
    "    align_id_pairs = list(decoded_results[0])\n",
    "    #print(align_id_pairs)\n",
    "    merged_sent2_align_id_pairs = merge_sent2_ids(align_id_pairs)\n",
    "    merged_sent1_align_id_pairs = merge_sent1_ids(merged_sent2_align_id_pairs)\n",
    "    merged_id_pairs = merge_align_ids_crossing(merged_sent1_align_id_pairs)\n",
    "    align_word_pairs = ids_to_words(merged_id_pairs, tokenized_sources[step], tokenized_targets[step])\n",
    "    print(merged_id_pairs, align_word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_distance(sent1, sent2, max_id=4999):\n",
    "    m = len(sent1)\n",
    "    n = len(sent2)\n",
    "    dp = [[0 for x in range(n+1)] for x in range(m+1)]\n",
    "    for i in range(m+1):\n",
    "        for j in range(n+1):\n",
    "            if i == 0:\n",
    "                dp[i][j] = j    # Min. operations = j\n",
    "            elif j == 0:\n",
    "                dp[i][j] = i    # Min. operations = i\n",
    "            elif sent1[i-1].lower() == sent2[j-1].lower():\n",
    "                dp[i][j] = dp[i-1][j-1]\n",
    "            else:\n",
    "                edit_candidates = np.array([\n",
    "                    dp[i][j-1], # Insert\n",
    "                    dp[i-1][j] # Remove\n",
    "                    ])\n",
    "                dp[i][j] = 1 + min(edit_candidates)\n",
    "    return dp\n",
    "\n",
    "def sent2edit(sent1, sent2):\n",
    "    dp = edit_distance(sent1, sent2)\n",
    "    edits = []\n",
    "    pos = []\n",
    "    m, n = len(sent1), len(sent2)\n",
    "    while m != 0 or n != 0:\n",
    "        curr = dp[m][n]\n",
    "        if m==0: #have to insert all here\n",
    "            while n>0:\n",
    "                left = dp[1][n-1]\n",
    "                edits.append(sent2[n-1])\n",
    "                pos.append(left)\n",
    "                n-=1\n",
    "        elif n==0:\n",
    "            while m>0:\n",
    "                top = dp[m-1][n]\n",
    "                edits.append('DEL')\n",
    "                pos.append(top)\n",
    "                m -=1\n",
    "        else: # we didn't reach any special cases yet\n",
    "            diag = dp[m-1][n-1]\n",
    "            left = dp[m][n-1]\n",
    "            top = dp[m-1][n]\n",
    "            if sent2[n-1].lower() == sent1[m-1].lower(): # keep\n",
    "                edits.append('KEEP')\n",
    "                pos.append(diag)\n",
    "                m -= 1\n",
    "                n -= 1\n",
    "            elif curr == top+1: # INSERT preferred before DEL\n",
    "                edits.append('DEL')\n",
    "                pos.append(top)  # (sent2[n-1])\n",
    "                m -= 1\n",
    "            else: #insert\n",
    "                edits.append(sent2[n - 1])\n",
    "                pos.append(left)  # (sent2[n-1])\n",
    "                n -= 1\n",
    "    edits = edits[::-1]\n",
    "    return edits\n",
    "\n",
    "\n",
    "def edit2sent(sent, edits, last=False):\n",
    "    new_sent = []\n",
    "    sent_pointer = 0 #counter the total of KEEP and DEL, then align with original sentence\n",
    "    if len(edits) == 0 or len(sent) ==0: # edit_list empty, return original sent\n",
    "        return sent\n",
    "    for i, edit in enumerate(edits):\n",
    "        if len(sent) > sent_pointer: #there are tokens left for editing\n",
    "            if edit ==\"KEEP\":\n",
    "                new_sent.append(sent[sent_pointer])\n",
    "                sent_pointer += 1\n",
    "            elif edit ==\"DEL\":\n",
    "                sent_pointer += 1\n",
    "            else: #insert the word in\n",
    "                new_sent.append(edit)\n",
    "    if sent_pointer < len(sent):\n",
    "        for i in range(sent_pointer,len(sent)):\n",
    "            new_sent.append(sent[i])\n",
    "    return new_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sent1 = \"Military experts say the line between combat is getting blurry.\"\n",
    "#sent2 = \"Military experts say war is changing.\"\n",
    "#sent1 = \"According to Ledford, Northrop executives said they would build substantial parts of the bomber in Palmdale, creating about 1,500 jobs.\"\n",
    "#sent2 = \"According to Ledford, Northrop said they would build most of the bomber parts in Palmdale. It would create 1,500 jobs.\"\n",
    "#sent1 = \"Their eyes are quite small, and their visual acuity is poor.\"\n",
    "#sent2 = \"Their eyes are very small, and they do not see well.\"\n",
    "sent1 = \"In return, Rollo swore fealty to Charles, converted to Christianity, and undertook to defend the northern region of France against the Incursions of other Viking groups.\"\n",
    "sent2 = \"Rollo swore to be loyal to Charles, then he changed his religion to Christianity. Rollo protected northern France by fighting Viking invaders.\"\n",
    "sent1 = \"A fee is the price one pays as remuneration for services, especially the Honorarium paid to a doctor, lawyer, consultant, or other member of a learned profession.\"\n",
    "sent2 = \"A price one might pay for services is a called a fee.\"\n",
    "sent1 = \"Thereafter the county's administration was conducted at Duns or Lauder until Greenlaw became the county town in 1596.\"\n",
    "sent2 = \"After that, the county offices were at Duns or Lauder. In 1596 they moved to Greenlaw.\"\n",
    "sent1 = \"MacGruber starts asking for simple objects to make something to defuse the bomb, but he is later distracted by something (usually involving his personal life) that makes him run out of time.\"\n",
    "sent2 = \"Macgruber starts by asking for simple objects to stop the bomb from working. later he is distracted by an event from his personal life. as a result, he runs out of time to stop the bomb.\"\n",
    "sent1 = \"As the largest sub-region in Mesoamerica, it encompassed a vast and varied landscape, from the mountainous regions of the Sierra Madre to the semi-arid plains of northern Yucatán.\"\n",
    "sent2 = \"As the largest sub-region in Mesoamerica, it was a vast and varied landscape.\"\n",
    "sent1 = 'The tongue is sticky because of the presence of glycoprotein-rich mucous, which both lubricates movement in and out of the snout and helps to catch ants and termites, which adhere to it.'\n",
    "sent2 = 'The sticky tongue helps to catch bugs.'\n",
    "sent1 = 'The polymer is most often epoxy, but other polymers, such as polyester, vinyl Ester or nylon, are also sometimes used.'\n",
    "sent2 = 'The most popular polymer to use is epoxy.'\n",
    "sent1 = 'Together they formed New Music Manchester, a group committed to contemporary music.'\n",
    "sent2 = 'Both of the formed a group committed to contemporary music called new music Manchester.'\n",
    "sent1_tok = nltk.word_tokenize(re.sub(r'[\\(\\)\\`\\'\\\"]', '', sent1))\n",
    "sent2_tok = nltk.word_tokenize(re.sub(r'[\\(\\)\\`\\'\\\"]', '', sent2))\n",
    "\n",
    "A = edit_distance(sent1_tok, sent2_tok, max_id=4999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = \"The International fight League was an American mixed martial arts( Mma) promotion billed as the world's first Mma League.\"\n",
    "sent2 = \"The International fight League was billed as the world's first mixed martial arts (Mma) League.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = \"Aside from this, Cameron has often worked in Christian-Themed productions, among them the Post-Rapture films left behind: the movie, left behind II: tribulation force, and left behind: world at war, in which he plays Cameron `` Buck'' Williams.\"\n",
    "sent2 = 'Cameron has often worked in Christian-Themed productions, among them are left behind: the movie, left behind II: tribulation force, and left behind: world at war, in which he plays Cameron \"Buck\" Williams.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = 'Characteristics Radar observations indicate a fairly pure iron-nickel composition.'\n",
    "sent2 = 'A mainly pure Iron-Nickel composition was observed by radar.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1_tok = nltk.word_tokenize(re.sub(r'[\\(\\)\\`\\'\\\"]', '', sent1))\n",
    "sent2_tok = nltk.word_tokenize(re.sub(r'[\\(\\)\\`\\'\\\"]', '', sent2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['KEEP', 'KEEP', 'by', 'KEEP', 'KEEP', 'KEEP', 'KEEP', 'KEEP', 'stop', 'DEL', 'DEL', 'DEL', 'DEL', 'KEEP', 'KEEP', 'from', 'working', '.', 'later', 'DEL', 'DEL', 'KEEP', 'KEEP', 'DEL', 'KEEP', 'KEEP', 'an', 'event', 'from', 'DEL', 'DEL', 'DEL', 'DEL', 'KEEP', 'KEEP', 'KEEP', '.', 'as', 'a', 'result', ',', 'he', 'runs', 'DEL', 'DEL', 'DEL', 'DEL', 'DEL', 'KEEP', 'KEEP', 'KEEP', 'to', 'stop', 'the', 'bomb', 'KEEP']\n",
      "['MacGruber', 'starts', 'by', 'asking', 'for', 'simple', 'objects', 'to', 'stop', 'the', 'bomb', 'from', 'working', '.', 'later', 'he', 'is', 'distracted', 'by', 'an', 'event', 'from', 'his', 'personal', 'life', '.', 'as', 'a', 'result', ',', 'he', 'runs', 'out', 'of', 'time', 'to', 'stop', 'the', 'bomb', '.']\n"
     ]
    }
   ],
   "source": [
    "B = sent2edit(sent1_tok, sent2_tok)\n",
    "print(B)\n",
    "print(edit2sent(sent1_tok,B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ad_spans(edits):\n",
    "    ad_spans = []\n",
    "    seen_a = [0 for i in range(len(edits))]\n",
    "    for i in range(len(edits) - 1):\n",
    "        if seen_a[i] != 1:\n",
    "            if edits[i] != 'KEEP' and edits[i] != 'DEL':\n",
    "                start = i\n",
    "                j = i + 1\n",
    "                flag = False\n",
    "                while j < len(edits):\n",
    "                    if edits[j] == 'DEL':\n",
    "                        j += 1\n",
    "                        flag = True\n",
    "                    elif edits[j] != 'KEEP' and edits[j] != 'DEL':\n",
    "                        if flag == False:\n",
    "                            seen_a[j] = 1\n",
    "                            j += 1\n",
    "                        else:\n",
    "                            break\n",
    "                    else:\n",
    "                        break\n",
    "                if flag == True:\n",
    "                    end = j - 1\n",
    "                    if end - start > 0:\n",
    "                        ad_spans.append((start, end))\n",
    "    return ad_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_d_starts_from_ad_spans(edits, ad_spans):\n",
    "    d_starts = []\n",
    "    for span in ad_spans:\n",
    "        a_start = span[0]\n",
    "        d_start = a_start\n",
    "        while d_start < len(edits):\n",
    "            if edits[d_start] != 'DEL':\n",
    "                d_start += 1\n",
    "            else:\n",
    "                break\n",
    "        d_starts.append(d_start)\n",
    "    return d_starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8, 12), (15, 20), (26, 32), (36, 47)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_ad_spans(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 19, 29, 43]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_d_starts(B, extract_ad_spans(B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_d_ids(edits):\n",
    "    d_ids = []\n",
    "    ad_spans = extract_ad_spans(edits)\n",
    "    d_start = extract_d_starts(edits, ad_spans)\n",
    "\n",
    "    cnt = 0\n",
    "    for i, j in zip(ad_spans, d_start):\n",
    "        d_ids.append((list(range(j, i[1]+1)), cnt))\n",
    "        cnt += 1\n",
    "    return d_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = \"MacGruber starts asking for simple objects to make something to defuse the bomb, but he is later distracted by something (usually involving his personal life) that makes him run out of time.\"\n",
    "sent2 = \"Macgruber starts by asking for simple objects to stop the bomb from working. later he is distracted by an event from his personal life. as a result, he runs out of time to stop the bomb.\"\n",
    "sent1_tok = nltk.word_tokenize(re.sub(r'[\\(\\)\\`\\'\\\"]', '', sent1))\n",
    "sent2_tok = nltk.word_tokenize(re.sub(r'[\\(\\)\\`\\'\\\"]', '', sent2))\n",
    "edits = sent2edit(sent1_tok, sent2_tok)\n",
    "ad_spans = extract_ad_spans(edits)\n",
    "aligns = [([21, 22], [21]), ([0], [0]), ([1], [1, 2]), ([2], [3]), ([3], [4]), ([4], [5]), ([5], [6]), ([6], [7]), ([10], [8]), ([11], [37]), ([12], [38]), ([13], [13]), ([15], [15]), ([16], [16]), ([17], [14]), ([18], [17]), ([19], [18]), ([20], [19, 20]), ([23], [22]), ([24], [23]), ([25], [24]), ([28], [30]), ([29], [31]), ([30], [32]), ([31], [33]), ([32], [34]), ([33], [39])] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MacGruber', 'starts', 'asking', 'for', 'simple', 'objects', 'to', 'make', 'something', 'to', 'defuse', 'the', 'bomb', ',', 'but', 'he', 'is', 'later', 'distracted', 'by', 'something', 'usually', 'involving', 'his', 'personal', 'life', 'that', 'makes', 'him', 'run', 'out', 'of', 'time', '.']\n",
      "['Macgruber', 'starts', 'by', 'asking', 'for', 'simple', 'objects', 'to', 'stop', 'the', 'bomb', 'from', 'working', '.', 'later', 'he', 'is', 'distracted', 'by', 'an', 'event', 'from', 'his', 'personal', 'life', '.', 'as', 'a', 'result', ',', 'he', 'runs', 'out', 'of', 'time', 'to', 'stop', 'the', 'bomb', '.']\n"
     ]
    }
   ],
   "source": [
    "print(sent1_tok)\n",
    "print(sent2_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_spans = extract_ad_spans(edits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_ids_to_edits(edits, sent2_tok):\n",
    "    edits_ids = []\n",
    "    sent1_pointer = 0\n",
    "    sent2_pointer = 0\n",
    "    for i in range(len(edits)):\n",
    "        if edits[i] == 'KEEP':\n",
    "            edits_ids.append(sent1_pointer)\n",
    "            sent1_pointer += 1\n",
    "        elif edits[i] == 'DEL':\n",
    "            edits_ids.append(sent1_pointer)\n",
    "            sent1_pointer += 1\n",
    "        else:\n",
    "            while sent2_pointer < len(sent2_tok):\n",
    "                if sent2_tok[sent2_pointer] == edits[i]:\n",
    "                    edits_ids.append(sent2_pointer)\n",
    "                    sent2_pointer += 1\n",
    "                    break\n",
    "                else:\n",
    "                    sent2_pointer += 1\n",
    "    return edits_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['KEEP', 'KEEP', 'by', 'KEEP', 'KEEP', 'KEEP', 'KEEP', 'KEEP', 'stop', 'DEL', 'DEL', 'DEL', 'DEL', 'KEEP', 'KEEP', 'from', 'working', '.', 'later', 'DEL', 'DEL', 'KEEP', 'KEEP', 'DEL', 'KEEP', 'KEEP', 'an', 'event', 'from', 'DEL', 'DEL', 'DEL', 'KEEP', 'KEEP', 'KEEP', '.', 'as', 'a', 'result', ',', 'he', 'runs', 'DEL', 'DEL', 'DEL', 'DEL', 'KEEP', 'KEEP', 'KEEP', 'to', 'stop', 'the', 'bomb', 'KEEP']\n",
      "[0, 1, 2, 2, 3, 4, 5, 6, 8, 7, 8, 9, 10, 11, 12, 11, 12, 13, 14, 13, 14, 15, 16, 17, 18, 19, 19, 20, 21, 20, 21, 22, 23, 24, 25, 25, 26, 27, 28, 29, 30, 31, 26, 27, 28, 29, 30, 31, 32, 35, 36, 37, 38, 33]\n"
     ]
    }
   ],
   "source": [
    "print(edits)\n",
    "print(assign_ids_to_edits(edits, sent2_tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = \"MacGruber starts asking for simple objects to make something to defuse the bomb, but he is later distracted by something (usually involving his personal life) that makes him run out of time.\"\n",
    "sent2 = \"Macgruber starts by asking for simple objects to stop the bomb from working. later he is distracted by an event from his personal life. as a result, he runs out of time to stop the bomb.\"\n",
    "sent1_tok = nltk.word_tokenize(re.sub(r'[\\(\\)\\`\\'\\\"]', '', sent1))\n",
    "sent2_tok = nltk.word_tokenize(re.sub(r'[\\(\\)\\`\\'\\\"]', '', sent2))\n",
    "edits = sent2edit(sent1_tok, sent2_tok)\n",
    "ad_spans = extract_ad_spans(edits)\n",
    "aligns = [([21, 22], [21]), ([0], [0]), ([1], [1, 2]), ([2], [3]), ([3], [4]), ([4], [5]), ([5], [6]), ([6], [7]), ([10], [8]), ([11], [37]), ([12], [38]), ([13], [13]), ([15], [15]), ([16], [16]), ([17], [14]), ([18], [17]), ([19], [18]), ([20], [19, 20]), ([23], [22]), ([24], [23]), ([25], [24]), ([28], [30]), ([29], [31]), ([30], [32]), ([31], [33]), ([32], [34]), ([33], [39])] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n",
      "54\n"
     ]
    }
   ],
   "source": [
    "print(len(edits))\n",
    "print(len(assign_ids_to_edits(edits, sent2_tok)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8, 12), (15, 20), (26, 31), (35, 45)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ad_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_splr_ids(edits, ad_spans, sent2_tok):\n",
    "    edits_ids = assign_ids_to_edits(edits, sent2_tok)\n",
    "    splr_ids = []\n",
    "    for ad_span_idx in range(len(ad_spans)):\n",
    "        splr_flag = False\n",
    "        for i in range(ad_spans[ad_span_idx][0], ad_spans[ad_span_idx][1] + 1):\n",
    "            if edits[i] == '.':\n",
    "                splr_flag = True\n",
    "        if splr_flag == True:\n",
    "            sent1_span = [edits_ids[j] for j in range(ad_spans[ad_span_idx][0], ad_spans[ad_span_idx][1]+1) if edits[j] == 'KEEP' or edits[j] == 'DEL']\n",
    "            sent2_span = [edits_ids[j] for j in range(ad_spans[ad_span_idx][0], ad_spans[ad_span_idx][1]+1) if edits[j] != 'KEEP' and edits[j] != 'DEL']\n",
    "            splr_ids.append((sent1_span, sent2_span, ad_span_idx))\n",
    "    return splr_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([13, 14], [11, 12, 13, 14], 1), ([26, 27, 28, 29], [25, 26, 27, 28, 29, 30, 31], 3)]\n"
     ]
    }
   ],
   "source": [
    "splr_ids = extract_splr_ids(edits, ad_spans, sent2_tok)\n",
    "print(splr_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3]\n"
     ]
    }
   ],
   "source": [
    "ad_spans_done = [i[2] for i in splr_ids]\n",
    "print(ad_spans_done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8, 12), (15, 20), (26, 31), (35, 45)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ad_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 19, 29, 42]\n"
     ]
    }
   ],
   "source": [
    "d_starts = extract_d_starts_from_ad_spans(edits, ad_spans)\n",
    "print(d_starts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['to', 'defuse', 'the', 'bomb']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent1_tok[9:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stop', 'DEL', 'DEL', 'DEL', 'DEL']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edits[8:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = \"MacGruber starts asking for simple objects to make something to defuse the bomb, but he is later distracted by something (usually involving his personal life) that makes him run out of time.\"\n",
    "sent2 = \"Macgruber starts by asking for simple objects to stop the bomb from working. later he is distracted by an event from his personal life. as a result, he runs out of time to stop the bomb.\"\n",
    "sent1_tok = nltk.word_tokenize(re.sub(r'[\\(\\)\\`\\'\\\"]', '', sent1))\n",
    "sent2_tok = nltk.word_tokenize(re.sub(r'[\\(\\)\\`\\'\\\"]', '', sent2))\n",
    "edits = sent2edit(sent1_tok, sent2_tok)\n",
    "ad_spans = extract_ad_spans(edits)\n",
    "aligns = [([21, 22], [21]), ([0], [0]), ([1], [1, 2]), ([2], [3]), ([3], [4]), ([4], [5]), ([5], [6]), ([6], [7]), ([10], [8]), ([11], [37]), ([12], [38]), ([13], [13]), ([15], [15]), ([16], [16]), ([17], [14]), ([18], [17]), ([19], [18]), ([20], [19, 20]), ([23], [22]), ([24], [23]), ([25], [24]), ([28], [30]), ([29], [31]), ([30], [32]), ([31], [33]), ([32], [34]), ([33], [39])] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_rep_ids(edits, ad_spans, sent2_tok, aligns, ad_spans_done):\n",
    "    edits_ids = assign_ids_to_edits(edits, sent2_tok)\n",
    "    d_starts = extract_d_starts_from_ad_spans(edits, ad_spans)\n",
    "    rep_ids = []\n",
    "    for ad_span_idx in range(len(ad_spans)):\n",
    "        if ad_span_idx in ad_spans_done:\n",
    "            continue\n",
    "        else:\n",
    "            now_span = ad_spans[ad_span_idx]\n",
    "            d_start_in_now_span = d_starts[ad_span_idx]\n",
    "            d_span_in_now_span = list(range(d_start_in_now_span, now_span[1]+1))\n",
    "            a_span_in_now_span = list(range(now_span[0], d_start_in_now_span))\n",
    "            sent1_ids_corresponding_d_span_in_now_span = [edits_ids[i] for i in d_span_in_now_span if edits[i] == 'DEL']\n",
    "            sent2_ids_corresponding_a_span_in_now_span = [edits_ids[i] for i in a_span_in_now_span if edits[i] != 'DEL']\n",
    "\n",
    "            added_words_to_sent1_by_a_span = []\n",
    "            for i in range(len(sent2_ids_corresponding_a_span_in_now_span)):\n",
    "                added_words_to_sent1_by_a_span.append(sent2_tok[sent2_ids_corresponding_a_span_in_now_span[i]])\n",
    "\n",
    "            aligned_words_in_sent2 = []\n",
    "            for i in range(len(sent1_ids_corresponding_d_span_in_now_span)):\n",
    "                for j in range(len(aligns)):\n",
    "                    if sent1_ids_corresponding_d_span_in_now_span[i] in aligns[j][0]:\n",
    "                        for aligned_word_id in aligns[j][1]:\n",
    "                            aligned_words_in_sent2.append(sent2_tok[aligned_word_id])\n",
    "\n",
    "            rep_flag = False\n",
    "            for word in aligned_words_in_sent2:\n",
    "                if word in added_words_to_sent1_by_a_span:\n",
    "                    rep_flag = True\n",
    "            \n",
    "            if rep_flag == True:\n",
    "                rep_ids.append((sent1_ids_corresponding_d_span_in_now_span, sent2_ids_corresponding_a_span_in_now_span, ad_span_idx))\n",
    "    \n",
    "    return rep_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([7, 8, 9, 10], [8], 0), ([20, 21, 22], [19, 20, 21], 2)]\n"
     ]
    }
   ],
   "source": [
    "rep_ids = extract_rep_ids(edits, ad_spans, sent2_tok, aligns, ad_spans_done)\n",
    "print(rep_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['make', 'something', 'to', 'defuse'] ['stop']\n"
     ]
    }
   ],
   "source": [
    "print(sent1_tok[7:11], sent2_tok[8:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['something', 'usually', 'involving'] ['an', 'event', 'from']\n"
     ]
    }
   ],
   "source": [
    "print(sent1_tok[20:23], sent2_tok[19:22])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['an', 'event', 'from', 'from']\n"
     ]
    }
   ],
   "source": [
    "test_d_span = [9, 10, 11, 12]\n",
    "test_d_span = [29, 30, 31]\n",
    "edits_ids = assign_ids_to_edits(edits, sent2_tok)\n",
    "sent1_span = [edits_ids[i] for i in test_d_span if edits[i] == 'DEL']\n",
    "aligned_words_in_sent2 = []\n",
    "for i in range(len(sent1_span)):\n",
    "    for j in range(len(aligns)):\n",
    "        if sent1_span[i] in aligns[j][0]:\n",
    "            for aligned_word_id in aligns[j][1]:\n",
    "                aligned_words_in_sent2.append(sent2_tok[aligned_word_id])\n",
    "print(aligned_words_in_sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = 'Characteristics Radar observations indicate a fairly pure iron-nickel composition.'\n",
    "sent2 = 'A mainly pure Iron-Nickel composition was observed by radar.'\n",
    "sent1_tok = nltk.word_tokenize(re.sub(r'[\\(\\)\\`\\'\\\"]', '', sent1))\n",
    "sent2_tok = nltk.word_tokenize(re.sub(r'[\\(\\)\\`\\'\\\"]', '', sent2))\n",
    "edits = sent2edit(sent1_tok, sent2_tok)\n",
    "ad_spans = extract_ad_spans(edits)\n",
    "aligns = [([1, 2], [8]), ([3], [6, 7]), ([4], [0]), ([5], [1]), ([6], [2]), ([7], [3]), ([8], [4]), ([9], [9])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DEL', 'DEL', 'DEL', 'DEL', 'KEEP', 'mainly', 'DEL', 'KEEP', 'KEEP', 'KEEP', 'was', 'observed', 'by', 'radar', 'KEEP']\n",
      "[(5, 6)]\n"
     ]
    }
   ],
   "source": [
    "print(edits)\n",
    "print(ad_spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_d_spans(edits):\n",
    "    d_spans = []\n",
    "    flag = False\n",
    "    seen = [0 for i in range(len(edits))]\n",
    "    for i in range(len(edits)):\n",
    "        if seen[i] != 1:\n",
    "            seen[i] = 1\n",
    "            if edits[i] != 'KEEP' and edits[i] != 'DEL':\n",
    "                flag = True\n",
    "            elif edits[i] == 'KEEP':\n",
    "                flag = False\n",
    "            else:\n",
    "                if flag == True:\n",
    "                    continue\n",
    "                else:\n",
    "                    start = i\n",
    "                    j = i + 1\n",
    "                    while j < len(edits):\n",
    "                        if edits[j] == 'DEL':\n",
    "                            seen[j] = 1\n",
    "                            j += 1\n",
    "                        elif edits[j] == 'KEEP':\n",
    "                            break\n",
    "                        else:\n",
    "                            flag = True\n",
    "                            break\n",
    "                    end = j - 1\n",
    "                    if end - start > 0:\n",
    "                        d_spans.append((start, end))\n",
    "    return d_spans        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 3)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_d_spans(edits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_a_spans(edits):\n",
    "    a_spans = []\n",
    "    seen_a = [0 for i in range(len(edits))]\n",
    "    for i in range(len(edits)):\n",
    "        if seen_a[i] != 1:\n",
    "            seen_a[i] = 1\n",
    "            if edits[i] != 'KEEP' and edits[i] != 'DEL':\n",
    "                start = i\n",
    "                j = i + 1\n",
    "                while j < len(edits):\n",
    "                    flag = False\n",
    "                    if edits[j] != 'KEEP' and edits[j] != 'DEL':\n",
    "                        seen_a[j] = 1\n",
    "                        j += 1\n",
    "                    elif edits[j] == 'DEL':\n",
    "                        flag = True\n",
    "                        break\n",
    "                    else:\n",
    "                        break\n",
    "                end = j - 1\n",
    "                if (flag == False) and (end - start >= 0):\n",
    "                    a_spans.append((start, end))\n",
    "    return a_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(10, 13)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_a_spans(edits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mvr_ids(edits, sent2_tok, aligns):\n",
    "    edits_ids = assign_ids_to_edits(edits, sent2_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_rep_span(edits, ad_spans, aligns, sent1_tok, sent2_tok, slp_spans):\n",
    "    d_starts = extract_d_starts(edits, ad_spans)\n",
    "    aligns_sent1_ids = [i[0] for i in aligns]\n",
    "    aligns_sent2_ids = [i[1] for i in aligns]\n",
    "    for span, d_start in zip(ad_spans, d_starts):\n",
    "        d_seen = [0 for i in range(len(d_start))]\n",
    "        a_wordlist = [sent2_tok[i] for i in range(span[0], d_start)]\n",
    "        edit_to_inserts = []\n",
    "        #d_pointer = d_start\n",
    "        for d_pointer in range(d_start, span[1]):\n",
    "            for i in range(len(aligns_sent1_ids)):\n",
    "                if d_pointer in aligns_sent1_ids[i] and d_seen[d_pointer - d_start] != 0:\n",
    "                    d_seen[d_pointer - d_start] = 1\n",
    "                    sent2_ids = aligns_sent2_ids[i]\n",
    "                    sent2_words = [sent2_tok[i] for i in sent2_ids]\n",
    "                    edit_to_insert = ['REP-S']\n",
    "                    for sent2_word in sent2_words:\n",
    "                        if sent2_word in a_wordlist:\n",
    "                            edit_to_insert.append(sent2_word)\n",
    "                    edit_to_insert.append('REP-E')\n",
    "                    if len(edit_to_insert) > 2:\n",
    "                        edit_to_inserts.append(edit_to_insert)\n",
    "\n",
    "\n",
    "        for i in range(len(aligns_sent1_ids)):\n",
    "            if d_pointer in aligns_sent1_ids[i] and d_seen[d_pointer - d_start] != 0:\n",
    "                d_seen[d_pointer - d_start] = 1\n",
    "                sent2_ids = aligns_sent2_ids[i]\n",
    "                sent2_words = [sent2_tok[i] for i in sent2_ids]\n",
    "                edit_to_insert = ['REP-S']\n",
    "                for sent2_word in sent2_words:\n",
    "                    if sent2_word in a_wordlist:\n",
    "                        edit_to_insert.append(sent2_word)\n",
    "                edit_to_insert.append('REP-E')\n",
    "                if len(edit_to_insert) > 2:\n",
    "                    edit_to_inserts.append(edit_to_insert)\n",
    "            d_pointer += 1\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8a7e92afde7a2b8cf7f9e7c82533dd48faf2b1d8196f2f255757b260712621ce"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('3.8.5': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
